<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Data pipeline using MongoDB and Kafka Connect on Kubernetes - Abhishek&#39;s dev blog</title><link rel="icon" type="image/png" href=icons/myicon.png /><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Data pipeline using MongoDB and Kafka Connect on Kubernetes" />
<meta property="og:description" content="How to build a simple data pipeline with MongoDB and Kafka with the MongoDB Kafka connectors which will be deployed on Kubernetes with Strimzi" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/blog/posts/mongodb-kafka-kubernetes/" />
<meta property="article:published_time" content="2020-05-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-05-12T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Data pipeline using MongoDB and Kafka Connect on Kubernetes"/>
<meta name="twitter:description" content="How to build a simple data pipeline with MongoDB and Kafka with the MongoDB Kafka connectors which will be deployed on Kubernetes with Strimzi"/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="/blogcss/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="/blogcss/main.css" />
	<link rel="stylesheet" type="text/css" href="/blogcss/custom.css" />
	<link rel="stylesheet" type="text/css" href="/blogcss/dark.css" media="(prefers-color-scheme: dark)" />
	<link rel="stylesheet" type="text/css" href="/blogcss/custom-dark.css" media="(prefers-color-scheme: dark)" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="/blogjs/main.js"></script>
	<script src="/blogjs/abc.js"></script>
	<script src="/blogjs/xyz.js"></script>
	<script src="https://code.jquery.com/jquery-3.4.1.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<base href="/blog">
	<h1 class="site-title"><a href="/blog">Abhishek&#39;s dev blog</a></h1>
	<div class="site-description"><h2>Mostly about Kafka, Databases, Kubernetes and other open source topics</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/abhirockzz/" title="Github"><i data-feather="github"></i></a><a href="https://twitter.com/abhi_tweeter" title="Twitter"><i data-feather="twitter"></i></a><a href="https://www.linkedin.com/in/abhirockzz/" title="LinkedIn"><i data-feather="linkedin"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="/blog/">Home</a>
			</li>
			
			<li>
				<a href="/blog/posts">All posts</a>
			</li>
			
			<li>
				<a href="/blog/about">About</a>
			</li>
			
			<li>
				<a href="/blog/books">Books</a>
			</li>
			
			<li>
				<a href="/blog/talks">Talks</a>
			</li>
			
			<li>
				<a href="/blog/tags">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Data pipeline using MongoDB and Kafka Connect on Kubernetes</h1>
			<div class="meta">Posted at &mdash; May 12, 2020</div>
		</div>

		<div class="markdown">
			<p>In <a href="https://dev.to/azure/kafka-connect-on-kubernetes-the-easy-way-2co9">Kafka Connect on Kubernetes, the easy way!</a>, I had demonstrated <a href="https://kafka.apache.org/documentation/#connect"><code>Kafka Connect</code></a> on <code>Kubernetes</code> using <a href="http://strimzi.io/"><code>Strimzi</code></a> along with the File source and sink connector. This blog will showcase how to build a simple data pipeline with MongoDB and Kafka with the <a href="https://docs.mongodb.com/kafka-connector/current/">MongoDB Kafka connectors</a> which will be deployed on Kubernetes with <code>Strimzi</code>.</p>
<p>I will be using the following Azure services:</p>
<blockquote>
<p>Please note that there are no hard dependencies on these components and the solution should work with alternatives as well</p>
</blockquote>
<ul>
<li><a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview?WT.mc_id=devto-blog-abhishgu">Azure Event Hubs for Apache Kafka</a> (any other Kafka cluster should work fine)</li>
<li><a href="https://docs.microsoft.com/azure/aks/?WT.mc_id=devto-blog-abhishgu">Azure Kubernetes Service</a> (feel free to use <code>minikube</code>, <code>kind</code> etc.)</li>
<li><a href="https://docs.microsoft.com/azure/cosmos-db/introduction?WT.mc_id=devto-blog-abhishgu">Azure Cosmos DB</a> as the MongoDB database, thanks to <a href="https://docs.microsoft.com/azure/cosmos-db/mongodb-introduction?WT.mc_id=devto-blog-abhishgu">Azure Cosmos DB&rsquo;s API for MongoDB</a></li>
</ul>
<p>In this tutorial, Kafka Connect components are being deployed to Kubernetes, but it is also applicable to any Kafka Connect deployment</p>
<p>What&rsquo;s covered?</p>
<ul>
<li>MongoDB Kafka Connector and Strimzi overview</li>
<li>Azure specific (optional) - Azure Event Hubs, Azure Cosmos DB and Azure Kubernetes Service</li>
<li>Setup and operate Source and Sink connectors</li>
<li>Test end to end scenario</li>
</ul>
<blockquote>
<p>All the artefacts are <a href="https://github.com/abhirockzz/mongodb-kafkaconnect-kubernetes">available on GitHub</a></p>
</blockquote>
<h2 id="overview">Overview</h2>
<p>Here is an overview of the different components:</p>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/ymtz9rdqag5dk391pol6.png" alt=""></p>
<blockquote>
<p>I have used a contrived/simple example in order to focus on the plumbing, moving parts</p>
</blockquote>
<h3 id="mongodb-kafka-connectors">MongoDB Kafka Connector(s)</h3>
<p>The MongoDB Kafka Connect integration provides two connectors: Source and Sink</p>
<ul>
<li>Source Connector: It pulls data from a <code>MongoDB</code> collection (that acts as a <code>source</code>) and writes them to Kafka topic</li>
<li>Sink connector: It is used to process the data in Kafka topic(s), persist them to another MongoDB collection (thats acts as a <code>sink</code>)</li>
</ul>
<blockquote>
<p>These connectors can be used independently as well, but in this blog, we will use them together to stitch the end-to-end solution</p>
</blockquote>
<h3 id="strimzi-overview"><code>Strimzi</code> overview</h3>
<p><code>Strimzi</code> simplifies the process of running Apache Kafka in a Kubernetes cluster by providing container images and Operators for running Kafka on Kubernetes. It is a <a href="https://strimzi.io/blog/2019/09/06/cncf/">part of the <code>Cloud Native Computing Foundation</code></a> as a <a href="https://www.cncf.io/sandbox-projects/"><code>Sandbox</code> project</a> (at the time of writing)</p>
<p><code>Strimzi Operators</code> are fundamental to the project. These Operators are purpose-built with specialist operational knowledge to effectively manage Kafka. Operators simplify the process of: Deploying and running Kafka clusters and components, Configuring and securing access to Kafka, Upgrading and managing Kafka and even taking care of managing topics and users.</p>
<h2 id="pre-requisites">Pre-requisites</h2>
<p><code>kubectl</code> - <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a></p>
<p>If you choose to use Azure Event Hubs, Azure Kubernetes Service or Azure Cosmos DB you will need a <a href="https://docs.microsoft.com/azure/?WT.mc_id=devto-blog-abhishgu">Microsoft Azure account</a>. Go ahead and <a href="https://azure.microsoft.com/free/?WT.mc_id=devto-blog-abhishgu">sign up for a free one!</a></p>
<p><code>Azure CLI</code> or <code>Azure Cloud Shell</code> - you can either choose to install the <a href="https://docs.microsoft.com/cli/azure/install-azure-cli?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu">Azure CLI</a> if you don&rsquo;t have it already (should be quick!) or just use the <a href="https://azure.microsoft.com/features/cloud-shell/?WT.mc_id=devto-blog-abhishgu">Azure Cloud Shell</a> from your browser.</p>
<h3 id="helm">Helm</h3>
<p>I will be using <code>Helm</code> to install <code>Strimzi</code>. Here is the documentation to install <code>Helm</code> itself - <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a></p>
<blockquote>
<p>You can also use the <code>YAML</code> files directly to install <code>Strimzi</code>. Check out the quick start guide here - <a href="https://strimzi.io/docs/quickstart/latest/#proc-install-product-str">https://strimzi.io/docs/quickstart/latest/#proc-install-product-str</a></p>
</blockquote>
<p>Let&rsquo;s start by setting up the required Azure services (if you&rsquo;re not using Azure, skip this section but please ensure you have the details for your Kafka cluster i.e. broker URLs and authentication credentials, if applicable)</p>
<blockquote>
<p>I recommend installing the below services as a part of a single <a href="https://docs.microsoft.com/azure/azure-resource-manager/management/overview?WT.mc_id=devto-blog-abhishgu#resource-groups">Azure Resource Group</a> which makes it easy to clean up these services</p>
</blockquote>
<h3 id="azure-cosmos-db">Azure Cosmos DB</h3>
<p>You need to create an Azure Cosmos DB account with the MongoDB API support enabled along with a Database and Collection. Follow these steps to setup Azure Cosmos DB using the Azure portal:</p>
<ul>
<li><a href="https://docs.microsoft.com/azure/cosmos-db/create-mongodb-java?WT.mc_id=devto-blog-abhishgu#create-a-database-account">Create an Azure Cosmos DB account</a></li>
<li><a href="https://docs.microsoft.com/azure/cosmos-db/create-mongodb-java?WT.mc_id=devto-blog-abhishgu#add-a-collection">Add a database and collection</a> and get the connection string</li>
</ul>
<blockquote>
<p>Learn more about how to <a href="https://docs.microsoft.com/azure/cosmos-db/databases-containers-items?WT.mc_id=devto-blog-abhishgu">Work with databases, containers, and items in Azure Cosmos DB</a></p>
</blockquote>
<p>If you want to use the Azure CLI or Cloud Shell, here is the sequence of commands which you need to execute:</p>
<p><a href="https://docs.microsoft.com/cli/azure/cosmosdb?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az-cosmosdb-create">Create an Azure Cosmos DB account</a> (notice <code>--kind MongoDB</code>)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">az cosmosdb create --resource-group &lt;RESOURCE_GROUP&gt; --name &lt;COSMOS_DB_NAME&gt; --kind MongoDB
</code></pre></div><p><a href="https://docs.microsoft.com/cli/azure/cosmosdb/mongodb/database?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az-cosmosdb-mongodb-database-create">Create the database</a></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">az cosmosdb mongodb database create --account-name &lt;COSMOS_DB_ACCOUN&gt; --name &lt;COSMOS_DB_NAME&gt; --resource-group &lt;RESOURCE_GROUP&gt;
</code></pre></div><p>Finally, <a href="https://docs.microsoft.com/cli/azure/cosmosdb/mongodb/collection?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az-cosmosdb-mongodb-collection-create">create a collection</a> within the database</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">az cosmosdb mongo collection create --account-name &lt;COSMOS_DB_ACCOUNT&gt; --database-name &lt;COSMOS_DB_NAME&gt; --name &lt;COSMOS_COLLECTION_NAME&gt; --resource-group-name &lt;RESOURCE_GROUP&gt; --shard &lt;SHARDING_KEY_PATH&gt;
</code></pre></div><p>Get the connection string and save it. You will be using it later</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">az cosmosdb list-connection-strings --name &lt;COSMOS_DB_ACCOUNT&gt; --resource-group &lt;RESOURCE_GROUP&gt; -o tsv --query connectionStrings[0].connectionString
</code></pre></div><p>Seed the collection with some data. There are many ways you could do this. For the purposes of this tutorial, I would recommend quick and easy, such as:</p>
<ul>
<li>The <code>Data Explorer</code> tab available in the Azure portal (when you create an Azure Cosmos DB account)</li>
<li><a href="https://docs.microsoft.com/azure/cosmos-db/data-explorer?WT.mc_id=devto-blog-abhishgu">Azure Cosmos DB explorer</a> (a standalone web-based interface )</li>
<li><a href="https://devblogs.microsoft.com/cosmosdb/preview-native-mongo-shell/">Native Mongo shell</a> (via the Data Explorer tab in Azure Portal)</li>
</ul>
<blockquote>
<p>Later on, when we deploy the source connector, we will double check to see if these (existing) items/records are picked up by the connector and sent to Kafka</p>
</blockquote>
<h3 id="azure-event-hubs">Azure Event Hubs</h3>
<p><a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-about?%5BWT.mc_id=devto-blog-abhishgu">Azure Event Hubs</a> is a data streaming platform and event ingestion service <a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview?WT.mc_id=devto-blog-abhishgu">and it also provides a Kafka endpoint</a> that can be used by  existing Kafka based applications as an alternative to running your own Kafka cluster. Event Hubs supports Apache Kafka protocol 1.0 and later, and works with existing Kafka client applications and other tools in the Kafka ecosystem including <code>Kafka Connect</code> (demonstrated in this blog),  <code>MirrorMaker</code> etc.</p>
<p>To setup an Azure Event Hubs cluster, you can choose from a variety of options including <a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-create?WT.mc_id=devto-blog-abhishgu">the Azure portal</a>, <a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-quickstart-cli?WT.mc_id=devto-blog-abhishgu">Azure CLI</a>, <a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-quickstart-powershell?WT.mc_id=devto-blog-abhishgu">Azure PowerShell</a> or <a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-resource-manager-namespace-event-hub?WT.mc_id=devto-blog-abhishgu">an ARM template</a>. Once the setup is complete, you will need the connection string (that will be used in subsequent steps) for <a href="https://docs.microsoft.com/azure/event-hubs/authenticate-shared-access-signature?WT.mc_id=devto-blog-abhishgu">authenticating to Event Hubs</a> - <a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-get-connection-string?WT.mc_id=devto-blog-abhishgu">use this guide</a> to finish this step.</p>
<p>Please ensure that you also create an Event Hub (same as a Kafka topic) to act as the target for our Kafka Connect connector (details in subsequent sections)</p>
<h3 id="azure-kubernetes-service">Azure Kubernetes Service</h3>
<p><a href="https://docs.microsoft.com/azure/aks/intro-kubernetes?WT.mc_id=devto-blog-abhishgu">Azure Kubernetes Service (AKS)</a> makes it simple to deploy a managed Kubernetes cluster in Azure. It reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. Here are examples of how you can setup an AKS cluster using <a href="https://docs.microsoft.com/azure/aks/kubernetes-walkthrough?WT.mc_id=devto-blog-abhishgu">Azure CLI</a>, <a href="https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal?WT.mc_id=devto-blog-abhishgu">Azure portal</a> or <a href="https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-rm-template?WT.mc_id=devto-blog-abhishgu">ARM template</a></p>
<p>Let&rsquo;s move on to the Kubernetes components now:</p>
<blockquote>
<p>Please note that I am re-using part of the sections from the <a href="https://dev.to/azure/kafka-connect-on-kubernetes-the-easy-way-2co9">previous blog post</a> (installation is the same after all!), but trying to keep it short at the same time to avoid repetition. For the parts that have been omitted e.g. explanation of the <code>Strimzi</code> component spec for Kafka Connect etc., I would request you to check out that blog</p>
</blockquote>
<h2 id="base-install">Base install</h2>
<p>To start off, we will install <code>Strimzi</code> and Kafka Connect, followed by the MongoDB connectors</p>
<h3 id="install-strimzi">Install Strimzi</h3>
<p>Installing Strimzi using <code>Helm</code> is pretty easy:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">//add helm chart repo for Strimzi
helm repo add strimzi https://strimzi.io/charts/

//install it! (I have used strimzi-kafka as the release name)
helm install strimzi-kafka strimzi/strimzi-kafka-operator
</code></pre></div><p>This will install the <code>Strimzi</code> Operator (which is nothing but a <code>Deployment</code>), Custom Resource Definitions and other Kubernetes components such as <code>Cluster Roles</code>, <code>Cluster Role Bindings</code> and <code>Service Accounts</code></p>
<p>For more details, <a href="https://github.com/strimzi/strimzi-kafka-operator/tree/master/helm-charts/strimzi-kafka-operator">check out this link</a></p>
<p>To confirm that the Strimzi Operator had been deployed, check it&rsquo;s <code>Pod</code> (it should transition to <code>Running</code> status after a while)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl get pods -l=name=strimzi-cluster-operator

NAME                                        READY   STATUS    RESTARTS   AGE
strimzi-cluster-operator-5c66f679d5-69rgk   1/1     Running   0          43s
</code></pre></div><p>Now that we have the &ldquo;brain&rdquo; (the Strimzi Operator) wired up, let&rsquo;s use it!</p>
<h3 id="kafka-connect">Kafka Connect</h3>
<p>We will need to create some helper Kubernetes components before we deploy Kafka Connect.</p>
<p>Clone the <a href="https://github.com/abhirockzz/mongodb-kafkaconnect-kubernetes">GitHub repo</a></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">git clone https://github.com/abhirockzz/mongodb-kafkaconnect-kubernetes
cd mongodb-kafkaconnect-kubernetes
</code></pre></div><p>Kafka Connect will need to reference an existing Kafka cluster (which in this case is Azure Event Hubs). We can store the authentication info for the cluster as a <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes <code>Secret</code></a> which can later be used in the Kafka Connect definition.</p>
<p>Update the <code>eventhubs-secret.yaml</code> file to include the credentials for Azure Event Hubs. Enter the connection string in the <code>eventhubspassword</code> attribute.</p>
<p>e.g.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">apiVersion: v1
kind: Secret
metadata:
  name: eventhubssecret
type: Opaque
stringData:
  eventhubsuser: $ConnectionString
  eventhubspassword: Endpoint=sb://&lt;eventhubs-namespace&gt;.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=&lt;access-key&gt;
</code></pre></div><blockquote>
<p>Leave <code>eventhubsuser: $ConnectionString</code> unchanged</p>
</blockquote>
<p>To create the <code>Secret</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl apply -f deploy/eventhubs-secret.yaml
</code></pre></div><p>Here is the Kafka Connect Strimzi definition:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">apiVersion</span>: kafka.strimzi.io/v1beta1
<span style="color:#268bd2">kind</span>: KafkaConnect
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">name</span>: my-connect-cluster
  <span style="color:#268bd2">annotations</span>:
    <span style="color:#268bd2">strimzi.io/use-connector-resources</span>: <span style="color:#2aa198">&#34;true&#34;</span>
<span style="color:#268bd2">spec</span>:
  <span style="color:#268bd2">image</span>: abhirockzz/strimzi-kafkaconnect-mongodb:latest
  <span style="color:#268bd2">version</span>: <span style="color:#2aa198">2.4.0</span>
  <span style="color:#268bd2">replicas</span>: <span style="color:#2aa198">1</span>
  <span style="color:#268bd2">bootstrapServers</span>: [EVENT_HUBS_NAMESPACE.servicebus.windows.net]:9093
  <span style="color:#268bd2">config</span>:
    <span style="color:#268bd2">group.id</span>: strimzi-connect-cluster
    <span style="color:#268bd2">offset.storage.topic</span>: mongo-connect-cluster-offsets
    <span style="color:#268bd2">config.storage.topic</span>: mongo-connect-cluster-configs
    <span style="color:#268bd2">status.storage.topic</span>: mongo-connect-cluster-status
  <span style="color:#268bd2">authentication</span>:
    <span style="color:#268bd2">type</span>: plain
    <span style="color:#268bd2">username</span>: $ConnectionString
    <span style="color:#268bd2">passwordSecret</span>:
      <span style="color:#268bd2">secretName</span>: eventhubssecret
      <span style="color:#268bd2">password</span>: eventhubspassword
  <span style="color:#268bd2">tls</span>:
    <span style="color:#268bd2">trustedCertificates</span>: []
</code></pre></div><p>I have used a custom Docker image to package the MongoDB Kafka connector. It uses the Strimzi Kafka image as (<code>strimzi/kafka</code>) the base</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">image</span>: abhirockzz/strimzi-kafkaconnect-mongodb:latest
</code></pre></div><blockquote>
<p>For details, check out <a href="https://strimzi.io/docs/latest/#creating-new-image-from-base-str">https://strimzi.io/docs/latest/#creating-new-image-from-base-str</a></p>
</blockquote>
<p>Here is the <code>Dockerfile</code> - you can tweak it, use a different one, upload to any Docker registry and reference that in the Kafka Connect manifest</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-dockerfile" data-lang="dockerfile"><span style="color:#719e07">FROM</span><span style="color:#2aa198"> strimzi/kafka:0.17.0-kafka-2.4.0</span>
<span style="color:#719e07">USER</span><span style="color:#2aa198"> root:root</span>
<span style="color:#719e07">COPY</span> ./connectors/ /opt/kafka/plugins/
<span style="color:#719e07">USER</span><span style="color:#2aa198"> 1001</span>
</code></pre></div><p>We are almost ready to create a Kafka Connect instance. Before that, make sure that you update the <code>bootstrapServers</code> property with the one for Azure Event Hubs endpoint e.g.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">spec:
  version: 2.4.0
  replicas: 1
  bootstrapServers: &lt;replace-with-eventhubs-namespace&gt;.servicebus.windows.net:9093
</code></pre></div><p>To create the Kafka Connect instance:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl apply -f deploy/kafka-connect.yaml
</code></pre></div><p>To confirm:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl get kafkaconnects

NAME                 DESIRED REPLICAS
my-connect-cluster   1
</code></pre></div><p>This will create a <code>Deployment</code> and a corresponding <code>Pod</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl get pod -l=strimzi.io/cluster=my-connect-cluster

NAME                                          READY   STATUS    RESTARTS   AGE
my-connect-cluster-connect-5bf9db5d9f-9ttg4   1/1     Running   0          1h
</code></pre></div><p>You have a Kafka Connect cluster in Kubernetes! Check out the logs using <code>kubectl logs &lt;pod name&gt;</code></p>
<blockquote>
<p>Check Azure Event Hubs - in the Azure Portal, open your Azure Event Hubs namespace and click on the Event Hubs tab, you should see Kafka Connect (internal) topics</p>
</blockquote>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/no3yf9a5hnz2sj9rwzuo.png" alt=""></p>
<h2 id="mongodb-kafka-connectors-1">MongoDB Kafka Connectors</h2>
<h3 id="source-connector">Source connector</h3>
<p>We will now setup the source connector. Here is the definition:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">apiVersion</span>: kafka.strimzi.io/v1alpha1
<span style="color:#268bd2">kind</span>: KafkaConnector
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">name</span>: mongodb-source-connector
  <span style="color:#268bd2">labels</span>:
    <span style="color:#268bd2">strimzi.io/cluster</span>: my-connect-cluster
<span style="color:#268bd2">spec</span>:
  <span style="color:#268bd2">class</span>: com.mongodb.kafka.connect.MongoSourceConnector
  <span style="color:#268bd2">tasksMax</span>: <span style="color:#2aa198">2</span>
  <span style="color:#268bd2">config</span>:
    <span style="color:#268bd2">connection.uri</span>: [AZURE_COSMOSDB_CONNECTION_STRING]
    <span style="color:#268bd2">topic.prefix</span>: mongo
    <span style="color:#268bd2">database</span>: [MONGODB_DATABASE_NAME]
    <span style="color:#268bd2">collection</span>: [MONGODB_COLLECTION_NAME]
    <span style="color:#268bd2">copy.existing</span>: <span style="color:#cb4b16">true</span>
    key.converter&#34;: org.apache.kafka.connect.json.JsonConverter
    <span style="color:#268bd2">key.converter.schemas.enable</span>: <span style="color:#cb4b16">false</span>
    <span style="color:#268bd2">value.converter</span>: org.apache.kafka.connect.json.JsonConverter
    <span style="color:#268bd2">value.converter.schemas.enable</span>: <span style="color:#cb4b16">false</span>
    <span style="color:#268bd2">publish.full.document.only</span>: <span style="color:#cb4b16">true</span>
    <span style="color:#268bd2">pipeline</span>: &gt;<span style="color:#2aa198">
</span><span style="color:#2aa198">      </span>      [{<span style="color:#2aa198">&#34;$match&#34;</span>:{<span style="color:#2aa198">&#34;operationType&#34;</span>:{<span style="color:#2aa198">&#34;$in&#34;</span>:[<span style="color:#2aa198">&#34;insert&#34;</span>,<span style="color:#2aa198">&#34;update&#34;</span>,<span style="color:#2aa198">&#34;replace&#34;</span>]}}},{<span style="color:#2aa198">&#34;$project&#34;</span>:{<span style="color:#2aa198">&#34;_id&#34;</span>:<span style="color:#2aa198">1</span>,<span style="color:#2aa198">&#34;fullDocument&#34;</span>:<span style="color:#2aa198">1</span>,<span style="color:#2aa198">&#34;ns&#34;</span>:<span style="color:#2aa198">1</span>,<span style="color:#2aa198">&#34;documentKey&#34;</span>:<span style="color:#2aa198">1</span>}}]
</code></pre></div><p>We use the label to refer to the kafka cluster we had just setup</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">name</span>: mongodb-source-connector
  <span style="color:#268bd2">labels</span>:
    <span style="color:#268bd2">strimzi.io/cluster</span>: my-connect-cluster
</code></pre></div><p>In the <code>config</code> section, we enter the connector config including the MongoDB connection string, database and collection names, whether we want to copy over existing data etc. The <code>topic.prefix</code> attribute is added to database &amp; collection names to generate the name of the Kafka topic to publish data to. e.g. if the database and collection names are <code>test_db</code>, <code>test_coll</code> respectively, then the Kafka topic name will be <code>mongo.test_db.test_coll</code>. Also, the <code>publish.full.document.only</code> is set to <code>true</code> - this means that, only the document which has been affected (created, updated, replaced) will be published to Kafka, and not the entire change stream document (which contains a lot of other info)</p>
<blockquote>
<p>For details, refer to the docs: <a href="https://docs.mongodb.com/kafka-connector/current/kafka-source/#source-connector-configuration-properties">https://docs.mongodb.com/kafka-connector/current/kafka-source/#source-connector-configuration-properties</a></p>
</blockquote>
<p>In addition to this, I want to highlight the <code>pipeline</code> attribute:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">    <span style="color:#268bd2">pipeline</span>: &gt;<span style="color:#2aa198">
</span><span style="color:#2aa198">      </span>      [{<span style="color:#2aa198">&#34;$match&#34;</span>:{<span style="color:#2aa198">&#34;operationType&#34;</span>:{<span style="color:#2aa198">&#34;$in&#34;</span>:[<span style="color:#2aa198">&#34;insert&#34;</span>,<span style="color:#2aa198">&#34;update&#34;</span>,<span style="color:#2aa198">&#34;replace&#34;</span>]}}},{<span style="color:#2aa198">&#34;$project&#34;</span>:{<span style="color:#2aa198">&#34;_id&#34;</span>:<span style="color:#2aa198">1</span>,<span style="color:#2aa198">&#34;fullDocument&#34;</span>:<span style="color:#2aa198">1</span>,<span style="color:#2aa198">&#34;ns&#34;</span>:<span style="color:#2aa198">1</span>,<span style="color:#2aa198">&#34;documentKey&#34;</span>:<span style="color:#2aa198">1</span>}}]
</code></pre></div><p>This is nothing but <code>JSON</code> (embedded within <code>YAML</code>.. what a joy!) which defines a custom pipeline. In case of the MongoDB API for Azure Cosmos DB, this is mandatory, due to the constraints in the Change Streams feature (at the time of writing). Please refer to this section in the [Azure Cosmos DB documentation] for  details</p>
<p>Let&rsquo;s do one last thing before deploying the connector. To confirm that our setup for the source connector is indeed working, we will need to keep an eye on the Kafka topic in Event Hubs</p>
<blockquote>
<p>Since we had specified <code>copy.existing: true</code> config for the connector, the existing items in the collection should be sent to the Kafka topic.</p>
</blockquote>
<p>There are many ways you can do this. <a href="https://docs.microsoft.com/azure/event-hubs/apache-kafka-developer-guide?WT.mc_id=devto-blog-abhishgu">This document</a> includes a lot of helpful links including, <a href="https://github.com/Azure/azure-event-hubs-for-kafka/tree/master/quickstart/kafkacat"><code>kafkacat</code></a>, <a href="https://github.com/Azure/azure-event-hubs-for-kafka/tree/master/quickstart/kafka-cli">Kafka CLI</a> etc.</p>
<blockquote>
<p>I will be using <code>kafkacat</code></p>
</blockquote>
<p>Install <code>kafkacat</code> - <a href="https://github.com/edenhill/kafkacat#install">https://github.com/edenhill/kafkacat#install</a> e.g. <code>brew install kafkacat</code> on mac. replace the properties in <code>kafkacat.conf</code> file (in the <code>GitHub</code> repo)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">metadata.broker.list<span style="color:#719e07">=[</span>EVENTHUBS_NAMESPACE<span style="color:#719e07">]</span>.servicebus.windows.net:9093
security.protocol<span style="color:#719e07">=</span>SASL_SSL
sasl.mechanisms<span style="color:#719e07">=</span>PLAIN
sasl.username<span style="color:#719e07">=</span><span style="color:#268bd2">$ConnectionString</span>
sasl.password<span style="color:#719e07">=</span><span style="color:#268bd2">Endpoint</span><span style="color:#719e07">=</span>sb://<span style="color:#719e07">[</span>EVENTHUBS_NAMESPACE<span style="color:#719e07">]</span>.servicebus.windows.net/;<span style="color:#268bd2">SharedAccessKeyName</span><span style="color:#719e07">=</span>RootManageSharedAccessKey;<span style="color:#268bd2">SharedAccessKey</span><span style="color:#719e07">=[</span>EVENTHUBS_ACCESS_KEY<span style="color:#719e07">]</span>
</code></pre></div><p>Export environment variables</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash"><span style="color:#b58900">export</span> <span style="color:#268bd2">KAFKACAT_CONFIG</span><span style="color:#719e07">=</span>kafkacat.conf
<span style="color:#b58900">export</span> <span style="color:#268bd2">BROKER</span><span style="color:#719e07">=[</span>EVENTHUBS_NAMESPACE<span style="color:#719e07">]</span>.servicebus.windows.net:9093
<span style="color:#b58900">export</span> <span style="color:#268bd2">TOPIC</span><span style="color:#719e07">=[</span>KAFKA_TOPIC e.g. mongo.test_db.test_coll<span style="color:#719e07">]</span>
</code></pre></div><p>The value for <code>TOPIC</code> follows a template, depending on the following connector config properties:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">&lt;topic.prefix&gt;.&lt;database&gt;.&lt;collection&gt;
</code></pre></div><p>&hellip; and invoke <code>kafkacat</code>:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kafkacat -b <span style="color:#268bd2">$BROKER</span> -t <span style="color:#268bd2">$TOPIC</span> -o beginning
</code></pre></div><p>In the connector manifest file, update the Azure Cosmos DB connection string, name of MongoDB database as well as collection</p>
<p>e.g.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">...
  <span style="color:#268bd2">connection.uri</span>: mongodb://&lt;COSMOSDB_ACCOUNT_NAME&gt;:&lt;COSMOSDB_PRIMARY_KEY&gt;@&lt;COSMOSDB_ACCOUNT_NAME&gt;.mongo.cosmos.azure.com:10255/?ssl=true&amp;replicaSet=globaldb&amp;maxIdleTimeMS=120000&amp;appName=@&lt;COSMOSDB_ACCOUNT_NAME&gt;@
  <span style="color:#268bd2">topic.prefix</span>: mongo
  <span style="color:#268bd2">database</span>: my_source_db
  <span style="color:#268bd2">collection</span>: my_source_coll
...
</code></pre></div><p>Ok, you&rsquo;re all set. From a different terminal, deploy the connector</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl apply -f deploy/mongodb-source-connector.yaml
</code></pre></div><p>To confirm, simply list the connectors:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl get kafkaconnectors

NAME                  AGE
mongodb-source-connector  70s
</code></pre></div><p>The connector should spin up and start weaving its magic. If you want to introspect the Kafka Connect logs:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">kubectl logs -f <span style="color:#719e07">$(</span>kubectl get pod -l<span style="color:#719e07">=</span>strimzi.io/cluster<span style="color:#719e07">=</span>my-connect-cluster -o <span style="color:#268bd2">jsonpath</span><span style="color:#719e07">=</span><span style="color:#2aa198">&#39;{.items[0].metadata.name}&#39;</span><span style="color:#719e07">)</span>
</code></pre></div><p>As per instructions, if you had created items in the source MongoDB collection, check the <code>kafkacat</code> terminal - you should see the Kafka topic records popping up. Go ahead and add a few more items to the MongoDB collection and confirm that you can see them in the <code>kafkacat</code> consumer terminal</p>
<blockquote>
<p>Resume feature: the connector has the ability to continue processing from a specific point in time. <a href="https://docs.mongodb.com/kafka-connector/current/kafka-source/#change-stream-event-document-format">As per connector docs</a> - <em>&ldquo;The top-level _id field is used as the resume token which is used to start a change stream from a specific point in time.&quot;</em></p>
</blockquote>
<h3 id="sink-connector">Sink connector</h3>
<p>We have the first half of the setup using which we can post MongoDB operations details to a Kafka topic. Let&rsquo;s finish the other half which will transform the data in the Kafka topic and store it in a destination MongoDB collection. For this, we will use the Sink connector - here is the definition</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">apiVersion</span>: kafka.strimzi.io/v1alpha1
<span style="color:#268bd2">kind</span>: KafkaConnector
<span style="color:#268bd2">metadata</span>:
  <span style="color:#268bd2">name</span>: mongodb-sink-connector
  <span style="color:#268bd2">labels</span>:
    <span style="color:#268bd2">strimzi.io/cluster</span>: my-connect-cluster
<span style="color:#268bd2">spec</span>:
  <span style="color:#268bd2">class</span>: com.mongodb.kafka.connect.MongoSinkConnector
  <span style="color:#268bd2">tasksMax</span>: <span style="color:#2aa198">2</span>
  <span style="color:#268bd2">config</span>:
    <span style="color:#268bd2">topics</span>: [EVENTHUBS_TOPIC_NAME]
    <span style="color:#268bd2">connection.uri</span>: [AZURE_COSMOSDB_CONNECTION_STRING]
    <span style="color:#268bd2">database</span>: [MONGODB_DATABASE_NAME]
    <span style="color:#268bd2">collection</span>: [MONGODB_COLLECTION_NAME]
    <span style="color:#268bd2">post.processor.chain</span>: com.mongodb.kafka.connect.sink.processor.DocumentIdAdder,com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder
    <span style="color:#268bd2">key.converter</span>: org.apache.kafka.connect.json.JsonConverter
    <span style="color:#268bd2">key.converter.schemas.enable</span>: <span style="color:#cb4b16">false</span>
    <span style="color:#268bd2">value.converter</span>: org.apache.kafka.connect.json.JsonConverter
    <span style="color:#268bd2">value.converter.schemas.enable</span>: <span style="color:#cb4b16">false</span>
</code></pre></div><p>In the <code>config</code> section we need to specify the source Kafka topic (using <code>topics</code>) - this is the same Kafka topic to which the source connector has written the records to. <code>database</code> and <code>collection</code> should be populated with the names of the destination database and collection respectively. Note the <code>post.processor.chain</code> attribute contains <code>com.mongodb.kafka.connect.sink.processor.KafkaMetaAdder</code> - this automatically adds an attribute (<code>topic-partition-offset</code>) to the MongoDB document and captures the Kafka topic, partition and offset values</p>
<p>e.g. <code>&quot;topic-partition-offset&quot; : &quot;mongo.test_db1.test_coll1-0-74&quot;,</code> where <code>mongo.test_db1.test_coll1</code> is the topic name, <code>0</code> is the partition and <code>74</code> is the offset</p>
<p>Before creating the sink connector, update the manifest with MongoDB connection string, name of the source Kafka topic as well as the sink database and collection</p>
<p>e.g.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">...
  <span style="color:#268bd2">config</span>:
    <span style="color:#268bd2">topics</span>: mongo.my_source_db.my_source_coll
    <span style="color:#268bd2">connection.uri</span>: mongodb://&lt;COSMOSDB_ACCOUNT_NAME&gt;:&lt;COSMOSDB_PRIMARY_KEY&gt;@&lt;COSMOSDB_ACCOUNT_NAME&gt;.mongo.cosmos.azure.com:10255/?ssl=true&amp;replicaSet=globaldb&amp;maxIdleTimeMS=120000&amp;appName=@&lt;COSMOSDB_ACCOUNT_NAME&gt;@
    <span style="color:#268bd2">database</span>: my_sink_db
    <span style="color:#268bd2">collection</span>: my_sink_coll
...
</code></pre></div><p>You can now deploy the connector:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl apply -f deploy/mongodb-sink-connector.yaml
</code></pre></div><p>To confirm, simply list the connectors:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">kubectl get kafkaconnectors

NAME                  AGE
mongodb-source-connector  70s
mongodb-sink-connector  70s
</code></pre></div><p>To start with, the connector copies over existing records in the Kafka topic (if any) into the sink collection. If you had initially created items in source Azure Cosmos DB collection, they should have been copied over to Kafka topic (by the source connector) and subsequently persisted to the sink Azure Cosmos DB collection by the sink connector - to confirm this, query Azure Cosmos DB using any of the methods mentioned previously</p>
<p>Here is a sample record (notice the <code>topic-partition-offset</code> attribute)</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#268bd2">&#34;_id&#34;</span> : ObjectId(<span style="color:#2aa198">&#34;5eb937e5a68a237befb2bd44&#34;</span>),
    <span style="color:#268bd2">&#34;name&#34;</span> : <span style="color:#2aa198">&#34;foo72&#34;</span>,
    <span style="color:#268bd2">&#34;email&#34;</span> : <span style="color:#2aa198">&#34;foo72@bar.com&#34;</span>,
    <span style="color:#268bd2">&#34;status&#34;</span> : <span style="color:#2aa198">&#34;online&#34;</span>,
    <span style="color:#268bd2">&#34;topic-partition-offset&#34;</span> : <span style="color:#2aa198">&#34;mongo.test_db1.test_coll1-0-74&#34;</span>,
    <span style="color:#268bd2">&#34;CREATE_TIME&#34;</span> : <span style="color:#2aa198">1589196724357</span>
}
</code></pre></div><blockquote>
<p>You can continue to experiment with the setup. Add, update and delete items in the source MongoDB collection and see the results&hellip;</p>
</blockquote>
<h2 id="clean-up">Clean up</h2>
<p>Once you are done exploring the application, you can delete the resources. If you placed Azure services (AKS, Event Hubs, Cosmos DB) under the same resource group, its easy executing a single command.</p>
<blockquote>
<p>Please be aware that this will delete all the resources in the group which includes the ones you created as part of the tutorial as well as any other service instances you might have if you used an <em>already existing</em> resource group</p>
</blockquote>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">az group delete --name <span style="color:#268bd2">$AZURE_RESOURCE_GROUP_NAME</span>
</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>As mentioned before, this was a simplified example to help focus on the different components and moving parts e.g. Kafka, Kubernetes, MongoDB, Kafka Connect etc. I demonstrated a use case where the record was modified before finally storing in the sink collection, but there are numerous other options which the connector offers, all of which are config based and do not require additional code (although the there are integration hooks as well). Some of the example include, using <a href="https://docs.mongodb.com/kafka-connector/current/kafka-source/#custom-pipeline-example">custom pipelines</a> in the source connector, [post-processors] (<a href="https://docs.mongodb.com/kafka-connector/current/kafka-sink-postprocessors/#post-processing-of-documents">https://docs.mongodb.com/kafka-connector/current/kafka-sink-postprocessors/#post-processing-of-documents</a>) in the sink connector etc.</p>
<h2 id="resources">Resources</h2>
<p>That&rsquo;s all for this blog. As always, stay tuned for more!</p>
<p>I&rsquo;ll leave you with a few resources:</p>
<ul>
<li>MongoDB Kafka Connector documentation - <a href="https://docs.mongodb.com/kafka-connector/current/">https://docs.mongodb.com/kafka-connector/current/</a></li>
<li>MongoDB Kafka Connector GitHub repo - <a href="https://github.com/mongodb/mongo-kafka">https://github.com/mongodb/mongo-kafka</a></li>
<li>Strimzi documentation - <a href="https://strimzi.io/docs/latest/">https://strimzi.io/docs/latest/</a></li>
<li>Kafka Connect - <a href="https://kafka.apache.org/documentation/#connect">https://kafka.apache.org/documentation/#connect</a></li>
</ul>

		</div>

		<div class="post-tags">
			
				
					<nav class="nav tags">
							<ul class="flat">
								
								<li><a href="/blog/tags/mongodb">mongodb</a></li>
								
								<li><a href="/blog/tags/kafka">kafka</a></li>
								
								<li><a href="/blog/tags/nosql">nosql</a></li>
								
								<li><a href="/blog/tags/kubernetes">kubernetes</a></li>
								
								<li><a href="/blog/tags/strimzi">strimzi</a></li>
								
							</ul>
					</nav>
				
			
		</div>
		</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> Â© Copyright notice |  <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54762029-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script>feather.replace()</script>
</body>
</html>
