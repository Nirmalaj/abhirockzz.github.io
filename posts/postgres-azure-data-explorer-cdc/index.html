<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Change Data Capture from PostgreSQL to Azure Data Explorer using Kafka Connect - Abhishek&#39;s blog - work in progress</title><link rel="icon" type="image/png" href=icons/myicon.png /><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:title" content="Change Data Capture from PostgreSQL to Azure Data Explorer using Kafka Connect" />
<meta property="og:description" content="This blog post demonstrates how you can use Change Data Capture to stream database modifications from PostgreSQL to Azure Data Explorer (Kusto) using Apache Kafka." />
<meta property="og:type" content="article" />
<meta property="og:url" content="./posts/postgres-azure-data-explorer-cdc/" />
<meta property="article:published_time" content="2020-11-02T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-11-02T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Change Data Capture from PostgreSQL to Azure Data Explorer using Kafka Connect"/>
<meta name="twitter:description" content="This blog post demonstrates how you can use Change Data Capture to stream database modifications from PostgreSQL to Azure Data Explorer (Kusto) using Apache Kafka."/>
<link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,300italic,400italic|Raleway:200,300" rel="stylesheet">

	<link rel="stylesheet" type="text/css" media="screen" href="./css/normalize.css" />
	<link rel="stylesheet" type="text/css" media="screen" href="./css/main.css" />
	<link rel="stylesheet" type="text/css" href="./css/custom.css" />
	<link rel="stylesheet" type="text/css" href="./css/dark.css" media="(prefers-color-scheme: dark)" />
	<link rel="stylesheet" type="text/css" href="./css/custom-dark.css" media="(prefers-color-scheme: dark)" />

	<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
	<script src="./js/main.js"></script>
	<script src="./js/abc.js"></script>
	<script src="./js/xyz.js"></script>
	<script src="https://code.jquery.com/jquery-3.4.1.js"></script>
</head>

<body>
	<div class="container wrapper post">
		<div class="header">
	<base href="./">
	<h1 class="site-title"><a href="./">Abhishek&#39;s blog - work in progress</a></h1>
	<div class="site-description"><h2>Mostly about Kafka, Databases, Kubernetes and other open source topics</h2><nav class="nav social">
			<ul class="flat"><a href="https://github.com/abhirockzz/" title="Github"><i data-feather="github"></i></a><a href="https://twitter.com/abhi_tweeter" title="Twitter"><i data-feather="twitter"></i></a><a href="https://www.linkedin.com/in/abhirockzz/" title="LinkedIn"><i data-feather="linkedin"></i></a></ul>
		</nav>
	</div>

	<nav class="nav">
		<ul class="flat">
			
			<li>
				<a href="./">Home</a>
			</li>
			
			<li>
				<a href="posts">All posts</a>
			</li>
			
			<li>
				<a href="about">About</a>
			</li>
			
			<li>
				<a href="books">Books</a>
			</li>
			
			<li>
				<a href="tags">Tags</a>
			</li>
			
		</ul>
	</nav>
</div>


		<div class="post-header">
			<h1 class="title">Change Data Capture from PostgreSQL to Azure Data Explorer using Kafka Connect</h1>
			<div class="meta">Posted at &mdash; Nov 2, 2020</div>
		</div>

		<div class="markdown">
			<p>This blog post demonstrates how you can use Change Data Capture to stream database modifications from PostgreSQL to <a href="https://docs.microsoft.com/en-us/azure/data-explorer/">Azure Data Explorer</a> (Kusto) using <a href="https://kafka.apache.org/">Apache Kafka</a>.</p>
<p>Change Data Capture (CDC) can be used to track row-level changes in database tables in response to create, update and delete operations. It is a powerful technique, but useful only when there is a way to leverage these events and make them available to other services.</p>
<h2 id="introduction">Introduction</h2>
<p>Using Apache Kafka, it is possible to convert traditional batched ETL processes into real-time, streaming mode. You can do-it-yourself (DIY) and write good old Kafka producer/consumer using a client SDK of your choice. But why would you do that when you&rsquo;ve Kafka Connect and it&rsquo;s suite of ready-to-use connectors?</p>
<p>Once you opt for Kafka Connect, you have a couple of options. One is the JDBC connector which basically polls the target database table(s) to get the information. There is a better (albeit, a little more complex) way based on change data capture. Enter <a href="https://debezium.io/">Debezium</a>, which is a distributed platform that builds on top of Change Data Capture features available in different databases. It provides a set of <a href="https://debezium.io/documentation/reference/1.2/connectors/index.html">Kafka Connect connectors</a> which tap into row-level changes in database table(s) and convert them into event streams that are sent to Apache Kafka. Once the change log events are in Kafka, they will be available to all the downstream applications.</p>
<p>Here is a high-level overview of the use-case presented in this post. It has been kept simplified for demonstration purposes.</p>
<h2 id="overview">Overview</h2>
<p>Data related to <code>Orders</code> is stored in the PostgreSQL database and contains information such as order ID, customer ID, city, transaction amount. time etc. This data is picked up the <a href="https://debezium.io/documentation/reference/1.2/connectors/postgresql.html">Debezium connector for PostgreSQL</a> and sent to a Kafka topic. Once the data is in Kafka, another (sink) connector sends them to Azure Data Explorer allow or further querying and analysis.</p>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/wttiquhcg9y82of7zmvm.jpg" alt=""></p>
<p>The individual components used in the end to end solution are as follows:</p>
<h3 id="source-and-destination">Source and Destination</h3>
<p>Data pipelines can be pretty complex! This blog post provides a simplified example where a PostgreSQL database will be used as the source of data and a Big Data analytics engine acts as the final destination (sink). Both these components run in Azure: <a href="https://docs.microsoft.com/azure/postgresql/?WT.mc_id=devto-blog-abhishgu">Azure Database for PostgreSQL</a> (the <em>Source</em>) is a relational database service based on the open-source <a href="https://www.postgresql.org/">Postgres</a> database engine and <a href="https://docs.microsoft.com/azure/data-explorer/?WT.mc_id=devto-blog-abhishgu">Azure Data Explorer</a> (the <em>Sink</em>) is a fast and scalable data exploration service that lets you collect, store, and analyze large volumes of data from any diverse sources, such as websites, applications, IoT devices, and more.</p>
<blockquote>
<p>Although Azure PostgreSQL DB has been used in this blog, the instructions should work for any Postgres database. So feel free to use alternate options if you&rsquo;d like!</p>
</blockquote>
<blockquote>
<p>The code and configuration associated with this blog post is available in this <a href="https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo">GitHub repository</a></p>
</blockquote>
<h3 id="kafka-and-kafka-connect">Kafka and Kafka Connect</h3>
<p>Apache Kafka along with Kafka Connect acts as a scalable platform for streaming data pipeline - the key components here are the source and sink connectors.</p>
<p>The Debezium connector for PostgreSQL captures row-level changes that insert, update, and delete database content and that were committed to a PostgreSQL database, generates data change event records and streams them to Kafka topics. Behind the scenes, it uses a combination of a Postgres output plugin (e.g. <code>wal2json</code>, <code>pgoutput</code> etc.) and the (Java) connector itself reads the changes produced by the output plug-in using the <a href="https://www.postgresql.org/docs/current/static/logicaldecoding-walsender.html">PostgreSQLâ€™s streaming replication protocol</a> and the <a href="https://github.com/pgjdbc/pgjdbc">JDBC driver</a>.</p>
<p>The <a href="https://github.com/Azure/kafka-sink-azure-kusto">Azure Data Explorer sink connector</a> picks up data from the configured Kafka topic, batches and sends them to Azure Data Explorer where they are queued up ingestion and eventually written to a table in Azure Data Explorer. The connector leverages the <a href="https://github.com/Azure/azure-kusto-java">Java SDK for Azure Data Explorer</a>.</p>
<p>Most of the components (except Azure Data Explorer and Azure PostgreSQL DB) run as Docker containers (using Docker Compose) - Kafka (and Zookeeper), Kafka Connect workers and the data generator application. Having said that, the instructions would work with any Kafka cluster and Kafka Connect workers, provided all the components are configured to access and communicate with each other as required. For example, you could have a Kafka cluster on <a href="https://docs.microsoft.com/azure/hdinsight/?WT.mc_id=devto-blog-abhishgu">Azure HD Insight</a> or <a href="https://www.confluent.io/blog/confluent-cloud-managed-kafka-service-azure-marketplace/">Confluent Cloud on Azure Marketplace</a>.</p>
<blockquote>
<p>Check out these <a href="https://github.com/Azure/azure-kusto-labs/tree/master/kafka-integration">hands-on labs</a> if you&rsquo;re interested in these scenarios</p>
</blockquote>
<p>Here is a breakdown of the components and their service definitions - you can refer to the complete <code>docker-compose</code> file <a href="https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo/blob/master/docker-compose.yaml">in the GitHub repo</a></p>
<h3 id="docker-compose-services">Docker Compose services</h3>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">  <span style="color:#268bd2">zookeeper</span>:
    <span style="color:#268bd2">image</span>: debezium/zookeeper:1.2
    <span style="color:#268bd2">ports</span>:
      - <span style="color:#2aa198">2181</span>:<span style="color:#2aa198">2181</span>
  <span style="color:#268bd2">kafka</span>:
    <span style="color:#268bd2">image</span>: debezium/kafka:1.2
    <span style="color:#268bd2">ports</span>:
      - <span style="color:#2aa198">9092</span>:<span style="color:#2aa198">9092</span>
    <span style="color:#268bd2">links</span>:
      - zookeeper
    <span style="color:#268bd2">depends_on</span>:
      - zookeeper
    <span style="color:#268bd2">environment</span>:
      - ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
</code></pre></div><p>The Kafka and Zookeeper run using the <a href="https://hub.docker.com/r/debezium/kafka/">debezium</a> images - they just work and are great for iterative development with quick feedback loop, demos etc.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#268bd2">dataexplorer-connector</span>:
    <span style="color:#268bd2">build</span>:
      <span style="color:#268bd2">context</span>: ./connector
      <span style="color:#268bd2">args</span>:
        <span style="color:#268bd2">KUSTO_KAFKA_SINK_VERSION</span>: <span style="color:#2aa198">1.0.1</span>
    <span style="color:#268bd2">ports</span>:
      - <span style="color:#2aa198">8080</span>:<span style="color:#2aa198">8083</span>
    <span style="color:#268bd2">links</span>:
      - kafka
    <span style="color:#268bd2">depends_on</span>:
      - kafka
    <span style="color:#268bd2">environment</span>:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=adx
      - CONFIG_STORAGE_TOPIC=adx_connect_configs
      - OFFSET_STORAGE_TOPIC=adx_connect_offsets
      - STATUS_STORAGE_TOPIC=adx_connect_statuses
  <span style="color:#268bd2">postgres-connector</span>:
    <span style="color:#268bd2">image</span>: debezium/connect:1.2
    <span style="color:#268bd2">ports</span>:
      - <span style="color:#2aa198">9090</span>:<span style="color:#2aa198">8083</span>
    <span style="color:#268bd2">links</span>:
      - kafka
    <span style="color:#268bd2">depends_on</span>:
      - kafka
    <span style="color:#268bd2">environment</span>:
      - BOOTSTRAP_SERVERS=kafka:9092
      - GROUP_ID=pg
      - CONFIG_STORAGE_TOPIC=pg_connect_configs
      - OFFSET_STORAGE_TOPIC=pg_connect_offsets
      - STATUS_STORAGE_TOPIC=pg_connect_statuses
</code></pre></div><p>The Kafka Connect source and sink connectors run as separate containers, just to make it easier for you to understand and reason about them - it is possible to run both the connectors in a single container as well.</p>
<p>Notice that, while the PostgreSQL connector is built into <a href="https://hub.docker.com/r/debezium/connect">debezium/connect</a> image, the Azure Data Explorer connector is setup using custom image. The <a href="https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo/blob/master/connector/Dockerfile">Dockerfile</a> is quite compact:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-dockerfile" data-lang="dockerfile"><span style="color:#719e07">FROM</span><span style="color:#2aa198"> debezium/connect:1.2</span>
<span style="color:#719e07">WORKDIR</span><span style="color:#2aa198"> $KAFKA_HOME/connect</span>
<span style="color:#719e07">ARG</span> KUSTO_KAFKA_SINK_VERSION
<span style="color:#719e07">RUN</span> curl -L -O https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v<span style="color:#268bd2">$KUSTO_KAFKA_SINK_VERSION</span>/kafka-sink-azure-kusto-<span style="color:#268bd2">$KUSTO_KAFKA_SINK_VERSION</span>-jar-with-dependencies.jar
</code></pre></div><p>Finally, the <code>orders-gen</code> service just <a href="https://golang.org/">Go</a> application to seed random orders data into PostgreSQL. You can refer to the Dockerfile in <a href="https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo/blob/master/orders-generator/Dockerfile">the GitHub repo</a></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml">  <span style="color:#268bd2">orders-gen</span>:
    <span style="color:#268bd2">build</span>:
      <span style="color:#268bd2">context</span>: ./orders-generator
    <span style="color:#268bd2">environment</span>:
      - PG_HOST=&lt;postgres host&gt;
      - PG_USER=&lt;postgres username&gt;
      - PG_PASSWORD=&lt;postgres password&gt;
      - PG_DB=&lt;postgres db name&gt;
</code></pre></div><p>Hopefully, by now you have a reasonable understanding of architecture and the components involved. Before diving into the practical aspects, you need take care of a few things.</p>
<h2 id="pre-requisites">Pre-requisites</h2>
<ul>
<li>You will need a <a href="https://docs.microsoft.com/azure/?product=featured&amp;WT.mc_id=devto-blog-abhishgu">Microsoft Azure account</a>. Don&rsquo;t worry, you can get it <a href="https://azure.microsoft.com/free/?WT.mc_id=devto-blog-abhishgu">for free</a> if you don&rsquo;t have one already!</li>
<li>Install <a href="https://docs.microsoft.com/cli/azure/install-azure-cli?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu">Azure CLI</a></li>
<li>Install <a href="https://docs.docker.com/get-docker/">Docker</a> and <a href="https://docs.docker.com/compose/install">Docker Compose</a></li>
</ul>
<p>Finally, clone this GitHub repo:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">git clone https://github.com/abhirockzz/kafka-adx-postgres-cdc-demo
<span style="color:#b58900">cd</span> kafka-adx-postgres-cdc-demo
</code></pre></div><p>To begin with, let&rsquo;s make sure you have setup and configured Azure Data Explorer and PostgreSQL database.</p>
<h2 id="setup-and-configure-azure-data-explorer">Setup and configure Azure Data Explorer</h2>
<ol>
<li>
<p>Create an Azure Data Explorer cluster and a database - <a href="https://docs.microsoft.com/azure/data-explorer/create-cluster-database-portal?WT.mc_id=devto-blog-abhishgu">this quickstart</a> will guide you through the process.</p>
</li>
<li>
<p>Create a table (<code>Orders</code>) and the mapping (<code>OrdersEventMapping</code>) using the <a href="https://docs.microsoft.com/azure/data-explorer/kql-quick-reference?WT.mc_id=devto-blog-abhishgu">KQL</a> queries below:</p>
</li>
</ol>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">.create table Orders (orderid: string, custid: string, city: string, amount: int, purchase_time: datetime)

.create table Orders ingestion json mapping &#39;OrdersEventMapping&#39; &#39;[{&#34;column&#34;:&#34;orderid&#34;,&#34;Properties&#34;:{&#34;path&#34;:&#34;$.orderid&#34;}},{&#34;column&#34;:&#34;custid&#34;,&#34;Properties&#34;:{&#34;path&#34;:&#34;$.custid&#34;}},{&#34;column&#34;:&#34;city&#34;,&#34;Properties&#34;:{&#34;path&#34;:&#34;$.city&#34;}},{&#34;column&#34;:&#34;amount&#34;,&#34;Properties&#34;:{&#34;path&#34;:&#34;$.amount&#34;}},{&#34;column&#34;:&#34;purchase_time&#34;,&#34;Properties&#34;:{&#34;path&#34;:&#34;$.purchase_time&#34;}}]&#39;
</code></pre></div><p>During the ingestion process, Azure Data Explorer attempts to optimize for throughput by batching small ingress data chunks together as they await ingestion - the <a href="https://docs.microsoft.com/azure/data-explorer/kusto/management/batchingpolicy?WT.mc_id=devto-blog-abhishgu">IngestionBatching policy</a> can be used to fine tune this process. Optionally, for the purposes of this demo, you can update the policy as such:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">.alter table Orders policy ingestionbatching @&#39;{&#34;MaximumBatchingTimeSpan&#34;:&#34;00:00:30&#34;, &#34;MaximumNumberOfItems&#34;: 500, &#34;MaximumRawDataSizeMB&#34;: 1024}&#39;

.show table &lt;enter database name&gt;.Orders policy ingestionbatching
</code></pre></div><blockquote>
<p>Refer to the <a href="https://docs.microsoft.com/azure/data-explorer/kusto/management/batching-policy?WT.mc_id=devto-blog-abhishgu">IngestionBatching policy command reference</a> for details</p>
</blockquote>
<ol start="3">
<li>Create a Service Principal in order for the connector to authenticate and connect to Azure Data Explorer service. If you want to use the Azure Portal to do this, please refer to <a href="https://docs.microsoft.com/azure/active-directory/develop/howto-create-service-principal-portal?WT.mc_id=devto-blog-abhishgu">How to: Use the portal to create an Azure AD application and service principal that can access resources</a>. The below example makes use of Azure CLI <a href="https://docs.microsoft.com/cli/azure/ad/sp?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az_ad_sp_create_for_rbac">az ad sp create-for-rbac</a> command. For example, to create a service principal with the name <code>adx-sp</code>:</li>
</ol>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">az ad sp create-for-rbac -n &#34;adx-sp&#34;
</code></pre></div><p>You will get a JSON response:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#268bd2">&#34;appId&#34;</span>: <span style="color:#2aa198">&#34;fe7280c7-5705-4789-b17f-71a472340429&#34;</span>,
  <span style="color:#268bd2">&#34;displayName&#34;</span>: <span style="color:#2aa198">&#34;kusto-sp&#34;</span>,
  <span style="color:#268bd2">&#34;name&#34;</span>: <span style="color:#2aa198">&#34;http://kusto-sp&#34;</span>,
  <span style="color:#268bd2">&#34;password&#34;</span>: <span style="color:#2aa198">&#34;29c719dd-f2b3-46de-b71c-4004fb6116ee&#34;</span>,
  <span style="color:#268bd2">&#34;tenant&#34;</span>: <span style="color:#2aa198">&#34;42f988bf-86f1-42af-91ab-2d7cd011db42&#34;</span>
}
</code></pre></div><blockquote>
<p>Please note down the <code>appId</code>, <code>password</code> and <code>tenant</code> as you will be using them in subsequent steps</p>
</blockquote>
<ol start="4">
<li>Add permissions to your database</li>
</ol>
<p>Provide appropriate role to the Service principal you just created. To assign the <code>admin</code> role, <a href="https://docs.microsoft.com/azure/data-explorer/manage-database-permissions?WT.mc_id=devto-blog-abhishgu#manage-permissions-in-the-azure-portal">follow this guide</a> to use the Azure portal or use the following command in your Data Explorer cluster</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">.add database &lt;enter database name&gt; admins  (&#39;aadapp=&lt;enter service principal appId&gt;;&lt;enter service principal tenant&gt;&#39;) &#39;AAD App&#39;
</code></pre></div><h2 id="setup-and-configure-azure-postgresql-db">Setup and configure Azure PostgreSQL DB</h2>
<p>You can setup PostgreSQL on Azure using a variety of options including, the <a href="https://docs.microsoft.com/azure/postgresql/quickstart-create-server-database-portal?WT.mc_id=devto-blog-abhishgu">Azure Portal</a>, <a href="https://docs.microsoft.com/azure/postgresql/quickstart-create-server-database-azure-cli?WT.mc_id=devto-blog-abhishgu">Azure CLI</a>, <a href="https://docs.microsoft.com/azure/postgresql/quickstart-create-postgresql-server-database-using-azure-powershell?WT.mc_id=devto-blog-abhishgu">Azure PowerShell</a>, <a href="https://docs.microsoft.com/azure/postgresql/quickstart-create-postgresql-server-database-using-arm-template?tabs=azure-portal&amp;WT.mc_id=devto-blog-abhishgu">ARM template</a>. Once you&rsquo;ve done that, you can easily connect to the database using you favourite programming language such as <a href="https://docs.microsoft.com/azure/postgresql/connect-java?WT.mc_id=devto-blog-abhishgu">Java</a>, <a href="https://docs.microsoft.com/azure/postgresql/connect-csharp?WT.mc_id=devto-blog-abhishgu">.NET</a>, <a href="https://docs.microsoft.com/azure/postgresql/connect-nodejs?WT.mc_id=devto-blog-abhishgu">Node.js</a>, <a href="https://docs.microsoft.com/azure/postgresql/connect-python?WT.mc_id=devto-blog-abhishgu">Python</a>, <a href="https://docs.microsoft.com/azure/postgresql/connect-go?WT.mc_id=devto-blog-abhishgu">Go</a> etc.</p>
<blockquote>
<p>Although the above references are for Single Server deployment mode, please note that <a href="https://docs.microsoft.com/azure/postgresql/overview?WT.mc_id=devto-blog-abhishgu#azure-database-for-postgresql---hyperscale-citus">Hyperscale (Citus) is another deployment mode you can use</a> for &ldquo;workloads that are approaching &ndash; or already exceed &ndash; 100 GB of data.&rdquo;</p>
</blockquote>
<p>Please ensure that you keep the following PostgreSQL related information handy since you will need them to configure the Debezium Connector in the subsequent sections - database hostname (and port), username, password</p>
<p>For the end-to-end solution to work as expected, we need to:</p>
<ul>
<li>Ensure that the PostgreSQL instance in Azure is accessible from the local Kafka Connect workers (containers)</li>
<li>Ensure appropriate PostrgeSQL replication setting (&ldquo;Logical&rdquo;)</li>
<li>Create the <code>Orders</code> table which you will use to try out the change data capture feature</li>
</ul>
<p>If you&rsquo;re using Azure DB for PostgreSQL, create a firewall rule using <a href="https://docs.microsoft.com/azure/postgresql/howto-manage-firewall-using-cli?WT.mc_id=devto-blog-abhishgu#create-firewall-rule">az postgres server firewall-rule create</a> command to whitelist your host. Since we&rsquo;re running Kafka Connect in Docker locally, simply navigate to the Azure portal (<strong>Connection security</strong> section of my PostrgreSQL instance) and choose <strong>Add current client IP address</strong> to make sure that your local IP is added to the firewall rule as such:</p>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/j0pocc5xlf82eaxfmzhg.png" alt=""></p>
<p>To change the replication mode for Azure DB for PostgreSQL, you can use the <a href="https://docs.microsoft.com/cli/azure/postgres/server/configuration?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az-postgres-server-configuration-set">az postgres server configuration</a> command:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">az postgres server configuration set --resource-group &lt;name of resource group&gt; --server-name &lt;name of server&gt; --name azure.replication_support --value logical
</code></pre></div><p>.. or use the <strong>Replication</strong> menu of your PostgreSQL instance in the Azure Portal:</p>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/nasgv52afr15hqzqzyyc.png" alt=""></p>
<p>After updating the configuration, you will need to re-start the server which you can do using the CLI (<a href="https://docs.microsoft.com/cli/azure/postgres/server?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az-postgres-server-restart">az postgres server restart</a>) or the portal.</p>
<p>Once the database is up and running, create the table. I have used <code>psql</code> CLI in this example, but feel free to use any other tool. For example, to connect to your PostgreSQL database on Azure over SSL (you will be prompted for the password):</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-bash" data-lang="bash">psql -h &lt;POSTGRESQL_INSTANCE_NAME&gt;.postgres.database.azure.com -p <span style="color:#2aa198">5432</span> -U &lt;POSTGRES_USER_NAME&gt; -W -d &lt;POSTGRES_DB_NAME&gt; --set<span style="color:#719e07">=</span><span style="color:#268bd2">sslmode</span><span style="color:#719e07">=</span>require

//example
psql -h my-pgsql.postgres.database.azure.com -p <span style="color:#2aa198">5432</span> -U foo@my-pgsql -W -d postgres --set<span style="color:#719e07">=</span><span style="color:#268bd2">sslmode</span><span style="color:#719e07">=</span>require
</code></pre></div><p>Use the below SQL to create the table:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"><span style="color:#719e07">CREATE</span> <span style="color:#719e07">SCHEMA</span> retail;

<span style="color:#719e07">CREATE</span> <span style="color:#719e07">TABLE</span> retail.orders_info (
	orderid <span style="color:#b58900">SERIAL</span> <span style="color:#719e07">NOT</span> <span style="color:#719e07">NULL</span> <span style="color:#719e07">PRIMARY</span> <span style="color:#719e07">KEY</span>,
	custid <span style="color:#b58900">INTEGER</span> <span style="color:#719e07">NOT</span> <span style="color:#719e07">NULL</span>,
	amount <span style="color:#b58900">INTEGER</span> <span style="color:#719e07">NOT</span> <span style="color:#719e07">NULL</span>,
	city <span style="color:#b58900">VARCHAR</span>(<span style="color:#2aa198">255</span>) <span style="color:#719e07">NOT</span> <span style="color:#719e07">NULL</span>,
	purchase_time <span style="color:#b58900">VARCHAR</span>(<span style="color:#2aa198">20</span>) <span style="color:#719e07">NOT</span> <span style="color:#719e07">NULL</span>
);
</code></pre></div><blockquote>
<p>The <code>purchase_time</code> captures the time when the purchase was executed, but it uses <code>VARCHAR</code> instead of a <code>TIMESTAMP</code> type (ideally) to reduce the overall complexity. This is because of the way <a href="https://debezium.io/documentation/reference/1.2/connectors/postgresql.html#postgresql-timestamp-type">Debezium Postgres connector treats TIMESTAMP data type</a> (and rightly so!)</p>
</blockquote>
<p>Over the course of the next few sections, you will setup the source (PostgreSQL), sink (Azure Data Explorer) connectors and validate the end to end pipeline.</p>
<h2 id="start-docker-containers">Start Docker containers</h2>
<p>Starting up our local environment is very easy, thanks to Docker Compose - all we need is a single command:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker-compose --project-name adx-kafka-cdc up --build
</code></pre></div><p>This will build (and start) the order generator application container along with Kafka, Zookeeper and Kafka Connect workers.</p>
<blockquote>
<p>It might take a while to download and start the containers: this is just a one time process.</p>
</blockquote>
<p>To confirm whether all the containers have started:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker-compose -p adx-kafka-cdc ps


//output

                 Name                              Command             State                      Ports                   
--------------------------------------------------------------------------------------------------------------------------
adx-kafka-cdc_dataexplorer-connector_1   /docker-entrypoint.sh start   Up      0.0.0.0:8080-&gt;8083/tcp, 8778/tcp, 9092/tcp,
                                                                               9779/tcp                                   
adx-kafka-cdc_kafka_1                    /docker-entrypoint.sh start   Up      8778/tcp, 0.0.0.0:9092-&gt;9092/tcp, 9779/tcp 
adx-kafka-cdc_orders-gen_1               /orders-gen                   Up                                                 
adx-kafka-cdc_postgres-connector_1       /docker-entrypoint.sh start   Up      0.0.0.0:9090-&gt;8083/tcp, 8778/tcp, 9092/tcp,
                                                                               9779/tcp                                   
adx-kafka-cdc_zookeeper_1                /docker-entrypoint.sh start   Up      0.0.0.0:2181-&gt;2181/tcp, 2888/tcp, 3888/tcp,
                                                                               8778/tcp, 9779/tcp
</code></pre></div><p>The orders generator app will start inserting random order events to the <code>orders_info</code> table in PostgreSQL. At this point you can also do quick sanity check to confirm that the order information is being persisted - I have used <a href="https://www.postgresql.org/docs/13/app-psql.html">psql</a> in the example below:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql">psql <span style="color:#719e07">-</span>h <span style="color:#719e07">&lt;</span>POSTGRESQL_INSTANCE_NAME<span style="color:#719e07">&gt;</span>.postgres.<span style="color:#719e07">database</span>.azure.com <span style="color:#719e07">-</span>p <span style="color:#2aa198">5432</span> <span style="color:#719e07">-</span>U <span style="color:#719e07">&lt;</span>POSTGRES_USER_NAME<span style="color:#719e07">&gt;</span> <span style="color:#719e07">-</span>W <span style="color:#719e07">-</span>d <span style="color:#719e07">&lt;</span>POSTGRES_DB_NAME<span style="color:#719e07">&gt;</span> <span style="color:#586e75">--set=sslmode=require
</span><span style="color:#586e75"></span>
<span style="color:#719e07">select</span> <span style="color:#719e07">*</span> <span style="color:#719e07">from</span> retail.orders_info <span style="color:#719e07">order</span> <span style="color:#719e07">by</span> orderid <span style="color:#719e07">desc</span> <span style="color:#719e07">limit</span> <span style="color:#2aa198">5</span>;
</code></pre></div><p>This will give you the five most recent orders:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sql" data-lang="sql"> orderid <span style="color:#719e07">|</span> custid <span style="color:#719e07">|</span> amount <span style="color:#719e07">|</span>   city    <span style="color:#719e07">|</span>    purchase_time    
<span style="color:#586e75">---------+--------+--------+-----------+---------------------
</span><span style="color:#586e75"></span>      <span style="color:#2aa198">10</span> <span style="color:#719e07">|</span>     <span style="color:#2aa198">77</span> <span style="color:#719e07">|</span>    <span style="color:#2aa198">140</span> <span style="color:#719e07">|</span> Seattle   <span style="color:#719e07">|</span> <span style="color:#2aa198">2020</span><span style="color:#719e07">-</span><span style="color:#2aa198">10</span><span style="color:#719e07">-</span><span style="color:#2aa198">09</span> <span style="color:#2aa198">07</span>:<span style="color:#2aa198">10</span>:<span style="color:#2aa198">49</span>
      <span style="color:#2aa198">9</span>  <span style="color:#719e07">|</span>    <span style="color:#2aa198">541</span> <span style="color:#719e07">|</span>    <span style="color:#2aa198">186</span> <span style="color:#719e07">|</span> Cleveland <span style="color:#719e07">|</span> <span style="color:#2aa198">2020</span><span style="color:#719e07">-</span><span style="color:#2aa198">10</span><span style="color:#719e07">-</span><span style="color:#2aa198">09</span> <span style="color:#2aa198">07</span>:<span style="color:#2aa198">10</span>:<span style="color:#2aa198">46</span>
      <span style="color:#2aa198">8</span>  <span style="color:#719e07">|</span>    <span style="color:#2aa198">533</span> <span style="color:#719e07">|</span>    <span style="color:#2aa198">116</span> <span style="color:#719e07">|</span> Cleveland <span style="color:#719e07">|</span> <span style="color:#2aa198">2020</span><span style="color:#719e07">-</span><span style="color:#2aa198">10</span><span style="color:#719e07">-</span><span style="color:#2aa198">09</span> <span style="color:#2aa198">07</span>:<span style="color:#2aa198">10</span>:<span style="color:#2aa198">42</span>
      <span style="color:#2aa198">7</span>  <span style="color:#719e07">|</span>    <span style="color:#2aa198">225</span> <span style="color:#719e07">|</span>    <span style="color:#2aa198">147</span> <span style="color:#719e07">|</span> Chicago   <span style="color:#719e07">|</span> <span style="color:#2aa198">2020</span><span style="color:#719e07">-</span><span style="color:#2aa198">10</span><span style="color:#719e07">-</span><span style="color:#2aa198">09</span> <span style="color:#2aa198">07</span>:<span style="color:#2aa198">10</span>:<span style="color:#2aa198">39</span>
      <span style="color:#2aa198">6</span>  <span style="color:#719e07">|</span>    <span style="color:#2aa198">819</span> <span style="color:#719e07">|</span>    <span style="color:#2aa198">184</span> <span style="color:#719e07">|</span> Austin    <span style="color:#719e07">|</span> <span style="color:#2aa198">2020</span><span style="color:#719e07">-</span><span style="color:#2aa198">10</span><span style="color:#719e07">-</span><span style="color:#2aa198">09</span> <span style="color:#2aa198">07</span>:<span style="color:#2aa198">10</span>:<span style="color:#2aa198">36</span>
(<span style="color:#2aa198">5</span> <span style="color:#719e07">rows</span>)
</code></pre></div><p>To stream the <code>orders</code> data to Kafka, we need to configure and start an instance of the Debezium PostgreSQL source connector.</p>
<h2 id="debezium-postgresql-source-connector-setup">Debezium PostgreSQL source connector setup</h2>
<p>Copy the JSON contents below to a file (you can name it <code>pg-source-config.json</code>). Please ensure that you update the following attributes with the values corresponding to your PostgreSQL instance: <code>database.hostname</code>, <code>database.user</code>, <code>database.password</code>.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#268bd2">&#34;name&#34;</span>: <span style="color:#2aa198">&#34;pg-orders-source&#34;</span>,
    <span style="color:#268bd2">&#34;config&#34;</span>: {
        <span style="color:#268bd2">&#34;connector.class&#34;</span>: <span style="color:#2aa198">&#34;io.debezium.connector.postgresql.PostgresConnector&#34;</span>,
        <span style="color:#268bd2">&#34;database.hostname&#34;</span>: <span style="color:#2aa198">&#34;&lt;enter database name&gt;.postgres.database.azure.com&#34;</span>,
        <span style="color:#268bd2">&#34;database.port&#34;</span>: <span style="color:#2aa198">&#34;5432&#34;</span>,
        <span style="color:#268bd2">&#34;database.user&#34;</span>: <span style="color:#2aa198">&#34;&lt;enter admin username&gt;@&lt;enter database name&gt;&#34;</span>,
        <span style="color:#268bd2">&#34;database.password&#34;</span>: <span style="color:#2aa198">&#34;&lt;enter admin password&gt;&#34;</span>,
        <span style="color:#268bd2">&#34;database.dbname&#34;</span>: <span style="color:#2aa198">&#34;postgres&#34;</span>,
        <span style="color:#268bd2">&#34;database.server.name&#34;</span>: <span style="color:#2aa198">&#34;myserver&#34;</span>,
        <span style="color:#268bd2">&#34;plugin.name&#34;</span>: <span style="color:#2aa198">&#34;wal2json&#34;</span>,
        <span style="color:#268bd2">&#34;table.whitelist&#34;</span>: <span style="color:#2aa198">&#34;retail.orders_info&#34;</span>,
        <span style="color:#268bd2">&#34;value.converter&#34;</span>: <span style="color:#2aa198">&#34;org.apache.kafka.connect.json.JsonConverter&#34;</span>
    }
}
</code></pre></div><p>At the time of writing, Debezium supports the following plugins: <code>decoderbufs</code>, <code>wal2json</code>, <code>wal2json_rds</code>, <code>wal2json_streaming</code>, <code>wal2json_rds_streaming</code> and <code>pgoutput</code>. I have used <code>wal2json</code> in this example, and it&rsquo;s <a href="https://docs.microsoft.com/azure/postgresql/concepts-logical?WT.mc_id=devto-blog-abhishgu">supported on Azure as well</a>.</p>
<p>To start the connector, simply use the Kafka Connect REST endpoint to submit the configuration.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X POST -H <span style="color:#2aa198">&#34;Content-Type: application/json&#34;</span> --data @pg-source-config.json http://localhost:9090/connectors

<span style="color:#586e75"># to confirm</span>
curl http://localhost:9090/connectors/pg-orders-source
</code></pre></div><blockquote>
<p>Notice that port for the REST endpoint is <code>9090</code> - this is per service port mapping defined in <code>docker-compose.yaml</code></p>
</blockquote>
<p>Let&rsquo;s peek into the Kafka topic and take a look at the change data capture events produced by the source connector.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker <span style="color:#b58900">exec</span> -it adx-kafka-cdc_kafka_1 bash
</code></pre></div><p>You will be dropped into a shell (inside the container). Execute the below command to consume the change data events from Kafka:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">cd bin &amp;&amp; ./kafka-console-consumer.sh --topic myserver.retail.orders_info --bootstrap-server kafka:9092 --from-beginning
</code></pre></div><blockquote>
<p>Note that the topic name <code>myserver.retail.orders_info</code> is as a result of the convention used by the Debezium connector</p>
</blockquote>
<p>Each event in topic is corresponding to a specific order. It is in a JSON format that looks like what&rsquo;s depicted below. Please note that the payload also contains the entire <code>schema</code> which has been removed for brevity.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#268bd2">&#34;schema&#34;</span>: {....},
    <span style="color:#268bd2">&#34;payload&#34;</span>: {
        <span style="color:#268bd2">&#34;before&#34;</span>: <span style="color:#cb4b16">null</span>,
        <span style="color:#268bd2">&#34;after&#34;</span>: {
            <span style="color:#268bd2">&#34;orderid&#34;</span>: <span style="color:#2aa198">51</span>,
            <span style="color:#268bd2">&#34;custid&#34;</span>: <span style="color:#2aa198">306</span>,
            <span style="color:#268bd2">&#34;amount&#34;</span>: <span style="color:#2aa198">183</span>,
            <span style="color:#268bd2">&#34;city&#34;</span>: <span style="color:#2aa198">&#34;Austin&#34;</span>,
            <span style="color:#268bd2">&#34;purchase_time&#34;</span>:<span style="color:#2aa198">&#34;2020-10-09 07:23:10&#34;</span>
        },
        <span style="color:#268bd2">&#34;source&#34;</span>: {
            <span style="color:#268bd2">&#34;version&#34;</span>: <span style="color:#2aa198">&#34;1.2.1.Final&#34;</span>,
            <span style="color:#268bd2">&#34;connector&#34;</span>: <span style="color:#2aa198">&#34;postgresql&#34;</span>,
            <span style="color:#268bd2">&#34;name&#34;</span>: <span style="color:#2aa198">&#34;myserver&#34;</span>,
            <span style="color:#268bd2">&#34;ts_ms&#34;</span>: <span style="color:#2aa198">1602057392691</span>,
            <span style="color:#268bd2">&#34;snapshot&#34;</span>: <span style="color:#2aa198">&#34;false&#34;</span>,
            <span style="color:#268bd2">&#34;db&#34;</span>: <span style="color:#2aa198">&#34;postgres&#34;</span>,
            <span style="color:#268bd2">&#34;schema&#34;</span>: <span style="color:#2aa198">&#34;retail&#34;</span>,
            <span style="color:#268bd2">&#34;table&#34;</span>: <span style="color:#2aa198">&#34;orders_info&#34;</span>,
            <span style="color:#268bd2">&#34;txId&#34;</span>: <span style="color:#2aa198">653</span>,
            <span style="color:#268bd2">&#34;lsn&#34;</span>: <span style="color:#2aa198">34220200</span>,
            <span style="color:#268bd2">&#34;xmin&#34;</span>: <span style="color:#cb4b16">null</span>
        },
        <span style="color:#268bd2">&#34;op&#34;</span>: <span style="color:#2aa198">&#34;c&#34;</span>,
        <span style="color:#268bd2">&#34;ts_ms&#34;</span>: <span style="color:#2aa198">1602057392818</span>,
        <span style="color:#268bd2">&#34;transaction&#34;</span>: <span style="color:#cb4b16">null</span>
    }
}
</code></pre></div><p>So far, we have the first half of our pipeline. Let&rsquo;s work on the second part!</p>
<h2 id="azure-data-explorer-sink-connector-setup">Azure Data Explorer sink connector setup</h2>
<p>Copy the JSON contents below to a file (you can name it <code>adx-sink-config.json</code>). Replace the values for the following attributes as per your Azure Data Explorer setup - <code>aad.auth.authority</code>, <code>aad.auth.appid</code>, <code>aad.auth.appkey</code>, <code>kusto.tables.topics.mapping</code> (the database name) and <code>kusto.url</code></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#268bd2">&#34;name&#34;</span>: <span style="color:#2aa198">&#34;adx-orders-sink&#34;</span>,
    <span style="color:#268bd2">&#34;config&#34;</span>: {
        <span style="color:#268bd2">&#34;connector.class&#34;</span>: <span style="color:#2aa198">&#34;com.microsoft.azure.kusto.kafka.connect.sink.KustoSinkConnector&#34;</span>,
        <span style="color:#268bd2">&#34;flush.size.bytes&#34;</span>: <span style="color:#2aa198">10000</span>,
        <span style="color:#268bd2">&#34;flush.interval.ms&#34;</span>: <span style="color:#2aa198">30000</span>,
        <span style="color:#268bd2">&#34;tasks.max&#34;</span>: <span style="color:#2aa198">2</span>,
        <span style="color:#268bd2">&#34;topics&#34;</span>: <span style="color:#2aa198">&#34;myserver.retail.orders_info&#34;</span>,
        <span style="color:#268bd2">&#34;kusto.tables.topics.mapping&#34;</span>: <span style="color:#2aa198">&#34;[{&#39;topic&#39;: &#39;myserver.retail.orders_info&#39;,&#39;db&#39;: &#39;&lt;enter database name&gt;&#39;, &#39;table&#39;: &#39;Orders&#39;,&#39;format&#39;: &#39;json&#39;, &#39;mapping&#39;:&#39;OrdersEventMapping&#39;}]&#34;</span>,
        <span style="color:#268bd2">&#34;aad.auth.authority&#34;</span>: <span style="color:#2aa198">&#34;&lt;enter tenant ID from service principal info&gt;&#34;</span>,
        <span style="color:#268bd2">&#34;kusto.url&#34;</span>: <span style="color:#2aa198">&#34;https://ingest-&lt;enter cluster name&gt;.&lt;enter region&gt;.kusto.windows.net&#34;</span>,
        <span style="color:#268bd2">&#34;aad.auth.appid&#34;</span>: <span style="color:#2aa198">&#34;&lt;enter app ID from service principal info&gt;&#34;</span>,
        <span style="color:#268bd2">&#34;aad.auth.appkey&#34;</span>: <span style="color:#2aa198">&#34;&lt;enter password from service principal info&gt;&#34;</span>,
        <span style="color:#268bd2">&#34;key.converter&#34;</span>: <span style="color:#2aa198">&#34;org.apache.kafka.connect.storage.StringConverter&#34;</span>,
        <span style="color:#268bd2">&#34;transforms&#34;</span>: <span style="color:#2aa198">&#34;unwrap&#34;</span>,
        <span style="color:#268bd2">&#34;transforms.unwrap.type&#34;</span>: <span style="color:#2aa198">&#34;io.debezium.transforms.ExtractNewRecordState&#34;</span>
    }
}
</code></pre></div><p>Notice that Kafka Connect <a href="https://kafka.apache.org/documentation/#connect_transforms">Single Message Transformation</a> (SMT) have been used here - this is the <code>ExtractNewRecordState</code> transformation that Debezium provides. You can read up on it <a href="https://debezium.io/documentation/reference/1.2/configuration/event-flattening.html">in the documentation</a></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json"><span style="color:#2aa198">&#34;transforms&#34;</span>: <span style="color:#2aa198">&#34;unwrap&#34;</span>,
<span style="color:#2aa198">&#34;transforms.unwrap.type&#34;</span>: <span style="color:#2aa198">&#34;io.debezium.transforms.ExtractNewRecordState&#34;</span>
</code></pre></div><p>It removes the <code>schema</code> and other parts from the JSON payload and keeps it down to only what&rsquo;s required. In this case, all we are looking for the order info from the <code>after</code> attribute (in the payload). For e.g.</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
    <span style="color:#268bd2">&#34;orderid&#34;</span>: <span style="color:#2aa198">51</span>,
    <span style="color:#268bd2">&#34;custid&#34;</span>: <span style="color:#2aa198">306</span>,
    <span style="color:#268bd2">&#34;amount&#34;</span>: <span style="color:#2aa198">183</span>,
    <span style="color:#268bd2">&#34;city&#34;</span>: <span style="color:#2aa198">&#34;Austin&#34;</span>,
    <span style="color:#268bd2">&#34;purchase_time&#34;</span>:<span style="color:#2aa198">&#34;2020-10-09 07:23:10&#34;</span>
}
</code></pre></div><p>You could model this differently of course (apply transformation in the source connector itself), but there are a couple of benefits to this approach:</p>
<ol>
<li>Only the relevant data sent to Azure Data Explorer</li>
<li>The Kafka topic contains the <em>entire</em> change data event (along with the schema) which can be leveraged by any downstream service</li>
</ol>
<p>To install the connector, just use the Kafka Connect REST endpoint like before:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">curl -X POST -H <span style="color:#2aa198">&#34;Content-Type: application/json&#34;</span> --data @adx-sink-config.json http://localhost:8080/connectors

<span style="color:#586e75"># check status</span>
curl http://localhost:8080/connectors/adx-orders-sink/status
</code></pre></div><blockquote>
<p>Notice that port for the REST endpoint is <code>8080</code> - this is per service port mapping defined in <code>docker-compose.yaml</code></p>
</blockquote>
<p>The connector should spin into action, authenticate to Azure Data Explorer and start batching ingestion processes.</p>
<blockquote>
<p>Note that <code>flush.size.bytes</code> and <code>flush.interval.ms</code> are used to regulate the batching process. Please refer to the <a href="https://github.com/Azure/kafka-sink-azure-kusto/blob/master/README.md#5-sink-properties">connector documentation</a> for details on the individual properties.</p>
</blockquote>
<p>Since the flush configuration for the connector and the batching policy for the <code>Orders</code> table in Azure Data Explorer is pretty aggressive (for demonstration purposes), you should see data flowing into Data Explorer quickly.</p>
<h2 id="query-data-explorer">Query Data Explorer</h2>
<p>You can query the Orders table in Data Explorer to slice and dice the data. Here are a few simple queries to start with.</p>
<p>Get details for orders from New York city;</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">Orders
| where city == &#39;New York&#39;
</code></pre></div><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/bbeogxflc7l139h264kz.png" alt=""></p>
<p>Get only the purchase amount and time for orders from New York city sorted by amount</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">Orders
| where city == &#39;New York&#39;
| project amount, purchase_time
| sort by amount
</code></pre></div><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/1tdj9trofkq5gf4qpgee.png" alt=""></p>
<p>Find out the average sales per city and represent that as a column chart:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">Orders
| summarize avg_sales = avg(amount) by city 
| render columnchart
</code></pre></div><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/g7ktf2qc06xj043vpvt0.png" alt=""></p>
<p>The total purchase amount per city, represented as a pie chart:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">Orders 
| summarize total = sum(amount) by city 
| sort by total
| render piechart 
</code></pre></div><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/pnwqqoaj7gaucw430bjb.png" alt=""></p>
<p>Number of orders per city, represented as a line chart:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">Orders
| summarize orders = count() by city
| sort by orders
| render linechart   
</code></pre></div><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/26w5grliaubh5zopjlfx.png" alt=""></p>
<p>How do purchases vary over a day?</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">Orders
| extend hour = floor(purchase_time % 1d , 10m)
| summarize event_count=count() by hour
| sort by hour asc
| render timechart
</code></pre></div><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/dsiuyo44alyla8f58bw9.png" alt=""></p>
<p>How does it vary over a day across different cities?</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">Orders
| extend hour= floor( purchase_time % 1d , 10m)
| where city in (&#34;New Delhi&#34;, &#34;Seattle&#34;, &#34;New York&#34;, &#34;Austin&#34;, &#34;Chicago&#34;, &#34;Cleveland&#34;)
| summarize event_count=count() by hour, city
| render columnchart
</code></pre></div><p><img src="https://dev-to-uploads.s3.amazonaws.com/i/eg3rrbot4n6kguwpwssx.png" alt=""></p>
<h3 id="azure-data-explorer-dashboards">Azure Data Explorer Dashboards</h3>
<ul>
<li>Learn how to <a href="https://docs.microsoft.com/en-us/azure/data-explorer/azure-data-explorer-dashboards">visualize data with Azure Data Explorer dashboards</a></li>
</ul>
<p><img src="https://dev-to-uploads.s3.amazonaws.com/i/q8xymgrw1wx76wzgptt3.png" alt=""></p>
<h2 id="clean-up">Clean up</h2>
<p>To stop the containers, you can:</p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">docker-compose -p adx-kafka-cdc down -v
</code></pre></div><p>To delete the Azure Data Explorer cluster/database, use <a href="https://docs.microsoft.com/cli/azure/kusto/cluster?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az-kusto-cluster-delete">az cluster delete</a> or <a href="https://docs.microsoft.com/cli/azure/kusto/database?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az-kusto-database-delete">az kusto database delete</a>. For PostgreSQL, simply use <a href="https://docs.microsoft.com/cli/azure/postgres/server?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu#az_postgres_server_delete">az postgres server delete</a></p>
<div class="highlight"><pre style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">az postgres server delete -g &lt;resource group name&gt; -n &lt;server name&gt;
az kusto cluster delete -n &lt;cluster name&gt; -g &lt;resource group name&gt;
az kusto database delete -n &lt;database name&gt; --cluster-name &lt;cluster name&gt; -g &lt;resource group name&gt;
</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>Kafka Connect helps you build scalable data pipelines without having to write custom plumbing code. You mostly need to setup, configure and of course operator the connectors. Remember that Kafka Connect worker instances are just JVM processes and depending on your scale and requirements you can use choose to operate them using <a href="https://docs.microsoft.com/azure/aks/?WT.mc_id=devto-blog-abhishgu">Azure Kubernetes Service</a>. Since Kafka Connect instances are stateless entities, you&rsquo;ve a lot of freedom in terms of the topology and sizing of your cluster workloads!</p>
<h2 id="additional-resources">Additional resources</h2>
<p>If you want to explore further, I would recommend</p>
<ul>
<li>Explore <a href="https://debezium.io/documentation/reference/1.2/connectors/index.html">Debezium connectors</a></li>
<li>Azure Data Explorer <a href="https://docs.microsoft.com/en-us/azure/data-explorer/ingest-data-overview">Ingestion overview</a></li>
<li>Explore <a href="https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/samples">what you can do with Kusto Query Language</a></li>
<li><a href="https://github.com/Azure/kafka-sink-azure-kusto/blob/master/README.md#3-features-supported">Data Explorer connector features</a></li>
<li>What is <a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-about?WT.mc_id=devto-blog-abhishgu">Azure Event Hubs</a>?</li>
<li>Use Change Data Capture with <a href="https://docs.microsoft.com/azure/event-hubs/event-hubs-kafka-connect-debezium?WT.mc_id=devto-blog-abhishgu">Kafka Connect support on Azure Event Hubs (Preview)</a></li>
</ul>

		</div>

		<div class="post-tags">
			
				
			
		</div>
		</div>
	<div class="footer wrapper">
	<nav class="nav">
		<div> Â© Copyright notice |  <a href="https://github.com/vividvilla/ezhil">Ezhil theme</a> | Built with <a href="https://gohugo.io">Hugo</a></div>
	</nav>
</div>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54762029-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script>feather.replace()</script>
</body>
</html>
