<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <title>Data Ingestion into Azure Data Explorer using Kafka Connect on Kubernetes - Foo Bar Baz</title>
  <meta name="description" content="In this blog, we will go over how to ingest data into Azure Data Explorer using the open source Kafka Connect Sink connector for Azure Data Explorer running on Kubernetes using Strimzi.">
  <meta name="author" content="Abhishek Gupta"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Foo Bar Baz",
    
    "url": "https:\/\/abhirockzz.github.io\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "https:\/\/abhirockzz.github.io\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "https:\/\/abhirockzz.github.io\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "https:\/\/abhirockzz.github.io\/posts\/kafka-azure-data-explorer-kubernetes\/",
          "name": "Data ingestion into azure data explorer using kafka connect on kubernetes"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "Abhishek Gupta"
  },
  "headline": "Data Ingestion into Azure Data Explorer using Kafka Connect on Kubernetes",
  "description" : "In this blog, we will go over how to ingest data into Azure Data Explorer using the open source Kafka Connect Sink connector for Azure Data Explorer running on Kubernetes using Strimzi.",
  "inLanguage" : "en",
  "wordCount":  1891 ,
  "datePublished" : "2020-09-25T00:00:00",
  "dateModified" : "2020-09-25T00:00:00",
  "image" : "https:\/\/abhirockzz.github.io\/icon.jpg",
  "keywords" : [ "" ],
  "mainEntityOfPage" : "https:\/\/abhirockzz.github.io\/posts\/kafka-azure-data-explorer-kubernetes\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "https:\/\/abhirockzz.github.io\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "https:\/\/abhirockzz.github.io\/icon.jpg",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>

<meta property="og:title" content="Data Ingestion into Azure Data Explorer using Kafka Connect on Kubernetes" />
<meta property="og:description" content="In this blog, we will go over how to ingest data into Azure Data Explorer using the open source Kafka Connect Sink connector for Azure Data Explorer running on Kubernetes using Strimzi.">
<meta property="og:image" content="https://abhirockzz.github.io/icon.jpg" />
<meta property="og:url" content="https://abhirockzz.github.io/posts/kafka-azure-data-explorer-kubernetes/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Foo Bar Baz" />

  <meta name="twitter:title" content="Data Ingestion into Azure Data Explorer using Kafka Connect on â€¦" />
  <meta name="twitter:description" content="In this blog, we will go over how to ingest data into Azure Data Explorer using the open source Kafka Connect Sink connector for Azure Data Explorer running on Kubernetes using Strimzi.">
  <meta name="twitter:image" content="https://abhirockzz.github.io/icon.jpg" />
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:site" content="@abhi_tweeter" />
  <meta name="twitter:creator" content="@abhi_tweeter" />
  <link href='https://abhirockzz.github.io/icons/myicon.png' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.80.0" />
  <link rel="alternate" href="https://abhirockzz.github.io/index.xml" type="application/rss+xml" title="Foo Bar Baz"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.5.0/css/all.css" integrity="sha384-B4dIYHKNBt8Bc12p+WXckhzcICo0wtJAoU8YZTY5qE0Id1GSseTk6S+L3BlXeVIU" crossorigin="anonymous">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><link rel="stylesheet" href="https://abhirockzz.github.io/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" /><link rel="stylesheet" href="https://abhirockzz.github.io/css/syntax.css" /><link rel="stylesheet" href="https://abhirockzz.github.io/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-54762029-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="https://abhirockzz.github.io/">Foo Bar Baz</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="Home" href="/">Home</a>
            </li>
          
        
          
            <li>
              <a title="All posts" href="/posts">All posts</a>
            </li>
          
        
          
            <li>
              <a title="About" href="/about">About</a>
            </li>
          
        
          
            <li>
              <a title="Books" href="/books">Books</a>
            </li>
          
        
          
            <li>
              <a title="Presentations" href="/presentations">Presentations</a>
            </li>
          
        
          
            <li>
              <a title="Tags" href="/tags">Tags</a>
            </li>
          
        

        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Foo Bar Baz" href="https://abhirockzz.github.io/">
            <img class="avatar-img" src="https://abhirockzz.github.io/icon.jpg" alt="Foo Bar Baz" />
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="posts-heading">
              
                <h1>Data Ingestion into Azure Data Explorer using Kafka Connect on Kubernetes</h1>
              
              
                <hr class="small">
              
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <p>In this blog, we will go over how to ingest data into <a href="https://docs.microsoft.com/azure/data-explorer/?WT.mc_id=devto-blog-abhishgu">Azure Data Explorer</a> using the open source <a href="https://github.com/Azure/kafka-sink-azure-kusto">Kafka Connect Sink connector for Azure Data Explorer</a> running on Kubernetes using Strimzi. <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a> is a tool for scalably and reliably streaming data between Apache Kafka and other systems using source and sink connectors and Strimzi provides a &ldquo;Kubernetes-native&rdquo; way of running Kafka clusters as well as Kafka Connect workers.</p>
<p>Azure Data Explorer is a fast and scalable data exploration service that lets you collect, store, and analyze large volumes of data from any diverse sources, such as websites, applications, IoT devices, and more. It has a rich connector ecosystem that supports ingestion into Azure Data Explorer as <a href="https://docs.microsoft.com/azure/data-explorer/ingest-data-overview?WT.mc_id=devto-blog-abhishgu">detailed here</a>. One of the supported sources is Apache Kafka and the sink connector allows you to move data from Kafka topics into Azure Data Explorer tables which you can later query and analyse. The best part is that you can do so in a scalable and fault tolerant way using just configuration!</p>
<p>Here is an overview of the scenario depicted in this blog post:</p>
<p><img src="https://strimzi.io/assets/images/posts/2020-09-22-overview.jpg" alt="Overview"></p>
<p>The Azure Data Explorer Kafka Connector picks up data from the configured Kafka topic and queues up ingestion processes (in batches) which eventually write data to a table in Azure Data Explorer. Behind the scenes, the connector leverages the <a href="https://github.com/Azure/azure-kusto-java">Java SDK for Azure Data Explorer</a>.</p>
<blockquote>
<p>Resources for this blog post are <a href="https://github.com/abhirockzz/kusto-kafka-connect-strimzi">available on GitHub</a></p>
</blockquote>
<h2 id="prerequisites">Prerequisites</h2>
<p>You will need an <a href="https://azure.microsoft.com/en-us/free/">Azure account</a> along with <a href="https://docs.microsoft.com/cli/azure/install-azure-cli?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu">Azure CLI</a> or <a href="https://docs.microsoft.com/azure/cloud-shell/quickstart?WT.mc_id=devto-blog-abhishgu">Azure Cloud Shell</a>.</p>
<p>Here are some quick pointers to setting up a Azure Data Explorer cluster and a managed Kubernetes service on Azure. I recommend installing the below services as a part of a single <a href="https://docs.microsoft.com/azure/azure-resource-manager/management/overview?WT.mc_id=devto-blog-abhishgu">Azure Resource Group</a> which makes it easy to manage these services</p>
<h3 id="azure-data-explorer">Azure Data Explorer</h3>
<p>You can setup an Azure Data Explorer cluster and database <a href="https://docs.microsoft.com/azure/data-explorer/create-cluster-database-portal?WT.mc_id=devto-blog-abhishgu">using Azure Portal</a>, <a href="https://docs.microsoft.com/azure/data-explorer/create-cluster-database-cli?WT.mc_id=devto-blog-abhishgu">Azure CLI</a> or any of the client SDKs such as <a href="https://docs.microsoft.com/azure/data-explorer/create-cluster-database-python?WT.mc_id=devto-blog-abhishgu">Python</a>. Once that&rsquo;s done, create a table (named <code>Storms</code>) and respective mapping (named <code>Storms_CSV_Mapping</code>) using below queries:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">.create table Storms <span class="o">(</span>StartTime: datetime, EndTime: datetime, EventId: int, State: string, EventType: string, Source: string<span class="o">)</span>

.create table Storms ingestion csv mapping <span class="s1">&#39;Storms_CSV_Mapping&#39;</span> <span class="s1">&#39;[{&#34;Name&#34;:&#34;StartTime&#34;,&#34;datatype&#34;:&#34;datetime&#34;,&#34;Ordinal&#34;:0}, {&#34;Name&#34;:&#34;EndTime&#34;,&#34;datatype&#34;:&#34;datetime&#34;,&#34;Ordinal&#34;:1},{&#34;Name&#34;:&#34;EventId&#34;,&#34;datatype&#34;:&#34;int&#34;,&#34;Ordinal&#34;:2},{&#34;Name&#34;:&#34;State&#34;,&#34;datatype&#34;:&#34;string&#34;,&#34;Ordinal&#34;:3},{&#34;Name&#34;:&#34;EventType&#34;,&#34;datatype&#34;:&#34;string&#34;,&#34;Ordinal&#34;:4},{&#34;Name&#34;:&#34;Source&#34;,&#34;datatype&#34;:&#34;string&#34;,&#34;Ordinal&#34;:5}]&#39;</span>
</code></pre></div><h3 id="azure-kubernetes-service-optional">Azure Kubernetes Service (optional)</h3>
<p>I have used <a href="https://docs.microsoft.com/azure/aks/?WT.mc_id=devto-blog-abhishgu">Azure Kubernetes Service</a> (AKS) but the instructions in this blog post should work for other options as well (e.g. with a local <code>minikube</code> cluster on your laptop). You can setup an AKS cluster using <a href="https://docs.microsoft.com/azure/aks/kubernetes-walkthrough?WT.mc_id=devto-blog-abhishgu">Azure CLI</a>, <a href="https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal?WT.mc_id=devto-blog-abhishgu">Azure portal</a> or <a href="https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-rm-template?WT.mc_id=devto-blog-abhishgu">ARM template</a></p>
<h2 id="base-installation">Base installation</h2>
<p>Start by installing the Strimzi Operator and use it to spin up a single-node Kafka Cluster on Kubernetes. Installing Strimzi using <code>Helm</code> is pretty easy:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">helm repo add strimzi https://strimzi.io/charts/
helm install strimzi-kafka strimzi/strimzi-kafka-operator
</code></pre></div><p>To confirm successful installation:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">kubectl get pods -l=name=strimzi-cluster-operator
</code></pre></div><p>You should see the cluster operator <code>Pod</code> in <code>Running</code> status</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">NAME                                        READY   STATUS    RESTARTS   AGE
strimzi-cluster-operator-5c66f679d5-69rgk   1/1     Running   <span class="m">0</span>          43s
</code></pre></div><p>To deploy a single-node kafka cluster (along with Zookeeper):</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/deploy/kafka.yaml
</code></pre></div><p>Wait for the cluster to start:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl get pod my-kafka-cluster-kafka-0 -w
</code></pre></div><p>The Kafka Pod should transition to <code>Running</code> status and both the containers should be in <code>READY</code> state</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">NAME                       READY   STATUS    RESTARTS   AGE
my-kafka-cluster-kafka-0   2/2     Running   <span class="m">0</span>          1m
</code></pre></div><h2 id="kafka-connect-cluster-setup">Kafka Connect cluster setup</h2>
<p>The Strimzi container images for Kafka Connect include two built-in file connectors - <code>FileStreamSourceConnector</code> and <code>FileStreamSinkConnector</code>. For the purposes of this blog, a custom Docker image seeded with <a href="https://github.com/Azure/kafka-sink-azure-kusto/releases/tag/v1.0.1">Azure Data Explorer connector</a> (version <code>1.0.1</code>) is <a href="https://hub.docker.com/r/abhirockzz/adx-connector-strimzi">available on Docker Hub</a> and it is referenced in the <code>KafkaConnect</code> resource definition (<code>image: abhirockzz/adx-connector-strimzi:1.0.1</code>):</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kafka.strimzi.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">KafkaConnect</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-connect-cluster</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">abhirockzz/adx-connector-strimzi:1.0.1</span><span class="w">
</span><span class="w">  </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="m">2.4.0</span><span class="w">
</span><span class="w"></span><span class="nn">...</span><span class="l">.</span><span class="w">
</span></code></pre></div><p>If you want to build your own Docker image, use the <a href="https://hub.docker.com/r/strimzi/kafka">Strimzi Kafka Docker image</a> as a base and add the Azure Data Explorer connector JAR top to the plugin path. Start by downloading the connector JAR file:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell"><span class="nb">export</span> <span class="nv">KUSTO_KAFKA_SINK_VERSION</span><span class="o">=</span>1.0.1
mkdir connector <span class="o">&amp;&amp;</span> <span class="nb">cd</span> connector
curl -L -O https://github.com/Azure/kafka-sink-azure-kusto/releases/download/v<span class="nv">$KUSTO_KAFKA_SINK_VERSION</span>/kafka-sink-azure-kusto-<span class="nv">$KUSTO_KAFKA_SINK_VERSION</span>-jar-with-dependencies.jar
</code></pre></div><p>Then, you can use this <code>Dockerfile</code> to build the Docker image:</p>
<div class="highlight"><pre class="chroma"><code class="language-dockerfile" data-lang="dockerfile"><span class="k">FROM</span><span class="s"> strimzi/kafka:0.19.0-kafka-2.4.0</span><span class="err">
</span><span class="err"></span><span class="k">USER</span><span class="s"> root:root</span><span class="err">
</span><span class="err"></span><span class="k">COPY</span> ./connector/ /opt/kafka/plugins/<span class="err">
</span><span class="err"></span><span class="k">RUN</span> ls -lrt /opt/kafka/plugins/<span class="err">
</span><span class="err"></span><span class="k">USER</span><span class="s"> 1001</span><span class="err">
</span></code></pre></div><blockquote>
<p>This technique has been illustrated in the <a href="https://strimzi.io/docs/operators/master/deploying.html#creating-new-image-from-base-str">Strimzi documentation</a></p>
</blockquote>
<h3 id="authentication">Authentication</h3>
<p>Before installing the connector, we need to create an <a href="https://docs.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals?WT.mc_id=devto-blog-abhishgu">Azure Service Principal</a> in order for the connector to authenticate and connect to Azure Data Explorer service. You can use the <a href="https://docs.microsoft.com/cli/azure/ad/sp?view=azure-cli-latest&amp;WT.mc_id=devto-blog-abhishgu">az ad sp create-for-rbac</a> command:</p>
<div class="highlight"><pre class="chroma"><code class="language-fallback" data-lang="fallback">az ad sp create-for-rbac -n &#34;kusto-sp&#34;
</code></pre></div><p>You will get a JSON response as below - please note down the <code>appId</code>, <code>password</code> and <code>tenant</code> as you will be using them in subsequent steps</p>
<div class="highlight"><pre class="chroma"><code class="language-json" data-lang="json"><span class="p">{</span>
  <span class="nt">&#34;appId&#34;</span><span class="p">:</span> <span class="s2">&#34;fe7280c7-5705-4789-b17f-71a472340429&#34;</span><span class="p">,</span>
  <span class="nt">&#34;displayName&#34;</span><span class="p">:</span> <span class="s2">&#34;kusto-sp&#34;</span><span class="p">,</span>
  <span class="nt">&#34;name&#34;</span><span class="p">:</span> <span class="s2">&#34;http://kusto-sp&#34;</span><span class="p">,</span>
  <span class="nt">&#34;password&#34;</span><span class="p">:</span> <span class="s2">&#34;29c719dd-f2b3-46de-b71c-4004fb6116ee&#34;</span><span class="p">,</span>
  <span class="nt">&#34;tenant&#34;</span><span class="p">:</span> <span class="s2">&#34;42f988bf-86f1-42af-91ab-2d7cd011db42&#34;</span>
<span class="p">}</span>
</code></pre></div><p><strong>Add permissions to your database</strong></p>
<p>Provide an appropriate role to the Service principal you just created. To assign the <code>admin</code> role, <a href="https://docs.microsoft.com/azure/data-explorer/manage-database-permissions?WT.mc_id=devto-blog-abhishgu#manage-permissions-in-the-azure-portal">follow this guide</a> to use the Azure portal or use the following command in your Data Explorer cluster</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">.add database &lt;database name&gt; admins  <span class="o">(</span><span class="s1">&#39;aadapp=&lt;service principal AppID&gt;;&lt;service principal TenantID&gt;&#39;</span><span class="o">)</span> <span class="s1">&#39;AAD App&#39;</span>
</code></pre></div><p>We will seed the auth related config as a <a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Secret</a> - later on you will see where this <code>Secret</code> is referenced.</p>
<p>Create a file called <code>adx-auth.yaml</code> with the below contents.</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Secret</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">adx-auth</span><span class="w">
</span><span class="w"></span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l">Opaque</span><span class="w">
</span><span class="w"></span><span class="nt">stringData</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">adx-auth.properties</span><span class="p">:</span><span class="w"> </span><span class="p">|-</span><span class="sd">
</span><span class="sd">    kustoURL: &lt;replace ADX Ingest URL&gt;
</span><span class="sd">    tenantID: &lt;enter service principal tenant ID&gt;
</span><span class="sd">    appID: &lt;enter service principal app ID&gt;
</span><span class="sd">    password: &lt;enter service principal tenant password&gt;</span><span class="w">    
</span></code></pre></div><p>Replace values for the following:</p>
<ul>
<li><code>kustoURL</code>: Azure Data Explorer ingestion URL e.g. <code>https://ingest-[cluster name].[region].kusto.windows.net</code></li>
<li><code>tenantID</code> - service principal tenant ID</li>
<li><code>appID</code> - service principal application ID</li>
<li><code>password</code> - service principal password</li>
</ul>
<h3 id="install-kafka-connect">Install Kafka Connect</h3>
<p>Create the <code>Secret</code> and initiate the Kafka Cluster creation:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f adx-auth.yaml

kubectl apply -f https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/deploy/kafka-connect.yaml
</code></pre></div><p>While you wait for the Kafka Connect cluster to start, take a look at this snippet of the <code>KafkaConnect</code> cluster resource definition. Notice the <code>externalConfiguration</code> attribute that points to the secret we had just created. It is loaded into the Kafka Connect <code>Pod</code> as a <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volume</a> and the Kafka <a href="https://kafka.apache.org/26/javadoc/org/apache/kafka/common/config/provider/FileConfigProvider.html">FileConfigProvider</a> is used to access them.</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kafka.strimzi.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">KafkaConnect</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">my-connect-cluster</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">abhirockzz/adx-connector-strimzi:1.0.1</span><span class="w">
</span><span class="w">  </span><span class="nt">config</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="l">...</span><span class="w">
</span><span class="w">    </span><span class="nt">config.providers</span><span class="p">:</span><span class="w"> </span><span class="l">file</span><span class="w">
</span><span class="w">    </span><span class="nt">config.providers.file.class</span><span class="p">:</span><span class="w"> </span><span class="l">org.apache.kafka.common.config.provider.FileConfigProvider</span><span class="w">
</span><span class="w">  </span><span class="nt">externalConfiguration</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">volumes</span><span class="p">:</span><span class="w">
</span><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">adx-auth-config</span><span class="w">
</span><span class="w">        </span><span class="nt">secret</span><span class="p">:</span><span class="w">
</span><span class="w">          </span><span class="nt">secretName</span><span class="p">:</span><span class="w"> </span><span class="l">adx-auth</span><span class="w">
</span></code></pre></div><p>To check Kafka Connect cluster status:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl get pod -l<span class="o">=</span>strimzi.io/cluster<span class="o">=</span>my-connect-cluster -w
</code></pre></div><p>Wait for the Kafka Connect Pod to transition into <code>Running</code> state.</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">NAME                                          READY   STATUS    RESTARTS   AGE
my-connect-cluster-connect-5bf9db5d9f-9ttg4   1/1     Running   <span class="m">0</span>          1m
</code></pre></div><h3 id="create-the-topic-and-install-connector">Create the topic and install connector</h3>
<p>You can use the <a href="https://strimzi.io/docs/operators/master/using.html#using-the-topic-operator-str">Strimzi Entity Operator</a> to create the <code>storm-events</code> topic. Here is the <code>Topic</code> definition:</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kafka.strimzi.io/v1beta1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">KafkaTopic</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">storm-events</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">strimzi.io/cluster</span><span class="p">:</span><span class="w"> </span><span class="l">my-kafka-cluster</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">partitions</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span></code></pre></div><p>To create:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/deploy/topic.yaml
</code></pre></div><p>Use <code>kubectl get kafkatopic</code> to see the topic you just created as well as internal Kafka topics</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">NAME                                                          PARTITIONS   REPLICATION FACTOR
consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a   <span class="m">50</span>           <span class="m">1</span>
storm-events                                                  <span class="m">3</span>            <span class="m">1</span>
strimzi-connect-cluster-configs                               <span class="m">1</span>            <span class="m">1</span>
strimzi-connect-cluster-offsets                               <span class="m">25</span>           <span class="m">1</span>
strimzi-connect-cluster-status                                <span class="m">5</span>            <span class="m">1</span>
</code></pre></div><p>Here is snippet of the connector (<code>KafkaConnector</code>) definition - it&rsquo;s just a way to capture configuration and metadata for the connector you want to install.</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">kafka.strimzi.io/v1alpha1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">KafkaConnector</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">adx-sink-connector</span><span class="w">
</span><span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">strimzi.io/cluster</span><span class="p">:</span><span class="w"> </span><span class="l">my-connect-cluster</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">class</span><span class="p">:</span><span class="w"> </span><span class="l">com.microsoft.azure.kusto.kafka.connect.sink.KustoSinkConnector</span><span class="w">
</span><span class="w">  </span><span class="nt">tasksMax</span><span class="p">:</span><span class="w"> </span><span class="m">3</span><span class="w">
</span><span class="w">  </span><span class="nt">config</span><span class="p">:</span><span class="w">
</span><span class="w">    </span><span class="nt">topics</span><span class="p">:</span><span class="w"> </span><span class="l">storm-events</span><span class="w">
</span><span class="w">    </span><span class="nt">flush.size.bytes</span><span class="p">:</span><span class="w"> </span><span class="m">10000</span><span class="w">
</span><span class="w">    </span><span class="nt">flush.interval.ms</span><span class="p">:</span><span class="w"> </span><span class="m">50000</span><span class="w">
</span><span class="w">    </span><span class="nt">kusto.tables.topics.mapping</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;[{&#39;topic&#39;: &#39;storm-events&#39;,&#39;db&#39;: &#39;[REPLACE DATABASE NAME]&#39;, &#39;table&#39;: &#39;Storms&#39;,&#39;format&#39;: &#39;csv&#39;, &#39;mapping&#39;:&#39;Storms_CSV_Mapping&#39;}]&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">kusto.url</span><span class="p">:</span><span class="w"> </span><span class="l">${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:kustoURL}</span><span class="w">
</span><span class="w">    </span><span class="nt">aad.auth.authority</span><span class="p">:</span><span class="w"> </span><span class="l">${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:tenantID}</span><span class="w">
</span><span class="w">    </span><span class="nt">aad.auth.appid</span><span class="p">:</span><span class="w"> </span><span class="l">${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:appID}</span><span class="w">
</span><span class="w">    </span><span class="nt">aad.auth.appkey</span><span class="p">:</span><span class="w"> </span><span class="l">${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:password}</span><span class="w">
</span><span class="w">    </span><span class="nt">key.converter</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;org.apache.kafka.connect.storage.StringConverter&#34;</span><span class="w">
</span><span class="w">    </span><span class="nt">value.converter</span><span class="p">:</span><span class="w"> </span><span class="s2">&#34;org.apache.kafka.connect.storage.StringConverter&#34;</span><span class="w">
</span></code></pre></div><blockquote>
<p>The <code>flush.size.bytes</code> and <code>flush.interval.ms</code> attributes work in tandem with each other and serve as a performance knob for batching. Please refer to the <a href="https://github.com/Azure/kafka-sink-azure-kusto/blob/master/README.md#5-sink-properties">connector GitHub repo</a> for details on these and other configuration parameters</p>
</blockquote>
<p>Notice how the individual properties (from the <code>Secret</code>) are actually referenced. For example to reference the Service Principal application ID, we used this:</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">aad.auth.appid</span><span class="p">:</span><span class="w"> </span><span class="l">${file:/opt/kafka/external-configuration/adx-auth-config/adx-auth.properties:appID}</span><span class="w">
</span></code></pre></div><ul>
<li><code>/opt/kafka/external-configuration</code> is a fixed path inside the container</li>
<li><code>adx-auth-config</code> is the name of the volume in the <code>KafkaConnect</code> definition</li>
<li><code>adx-auth.properties</code> is the name of the file as defined in the <code>Secret</code></li>
<li><code>appID</code> is the name of key</li>
</ul>
<blockquote>
<p>The direct attribute name has been used to define non-sensitive connector configs (e.g. <code>topics: storm-events</code>). Alternatively, can encapsulate these in a <code>ConfigMap</code>, load them as a <code>Volume</code> and reference them (just like the sensitive attributes using a <code>Secret</code>).</p>
</blockquote>
<p>Copy the above definition for the <code>KafkaConnector</code> to local file <code>adx-connect-config.yaml</code>. Make sure you replace the correct database name in the <code>kusto.tables.topics.mapping</code> attribute. To create:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f adx-connect-config.yaml
</code></pre></div><p>Check the kafka connect logs <code>kubectl logs -l=strimzi.io/cluster=my-connect-cluster</code>. If everything is working fine, you should see logs similar to this:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">....
INFO <span class="o">[</span>Consumer <span class="nv">clientId</span><span class="o">=</span>connector-consumer-adx-sink-connector-1, <span class="nv">groupId</span><span class="o">=</span>connect-adx-sink-connector<span class="o">]</span> Resetting offset <span class="k">for</span> partition storm-events-1 to offset 0. <span class="o">(</span>org.apache.kafka.clients.consumer.internals.SubscriptionState<span class="o">)</span> <span class="o">[</span>task-thread-adx-sink-connector-1<span class="o">]</span>

INFO <span class="o">[</span>Consumer <span class="nv">clientId</span><span class="o">=</span>connector-consumer-adx-sink-connector-2, <span class="nv">groupId</span><span class="o">=</span>connect-adx-sink-connector<span class="o">]</span> Resetting offset <span class="k">for</span> partition storm-events-2 to offset 0. <span class="o">(</span>org.apache.kafka.clients.consumer.internals.SubscriptionState<span class="o">)</span> <span class="o">[</span>task-thread-adx-sink-connector-2<span class="o">]</span>
</code></pre></div><h2 id="data-ingestion-in-action">Data ingestion in action</h2>
<p>So, we have everything setup. All we need is events to be sent to the Kafka topic, so that we can see the connector in action and ingest data into Azure Data Explorer.</p>
<p>You can use this handy event generator application (available in <a href="https://hub.docker.com/r/abhirockzz/adx-event-producer">Docker Hub</a>) and deploy it to your Kubernetes cluster - the <code>Dockerfile</code> is available in the <a href="https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/storm-events-producer/Dockerfile">GitHub repo</a> in case you want to reference it.</p>
<p>Kubernetes <code>Deployment</code> snippet:</p>
<div class="highlight"><pre class="chroma"><code class="language-yaml" data-lang="yaml"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">apps/v1</span><span class="w">
</span><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">Deployment</span><span class="w">
</span><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">adx-event-producer</span><span class="w">
</span><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="w">  </span><span class="l">....</span><span class="w">
</span><span class="w">    </span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span><span class="w">      </span><span class="nt">containers</span><span class="p">:</span><span class="w">
</span><span class="w">        </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">adx-event-producer</span><span class="w">
</span><span class="w">          </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l">abhirockzz/adx-event-producer</span><span class="w">
</span><span class="w">          </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l">Always</span><span class="w">
</span><span class="w">          </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span><span class="w">            </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">KAFKA_BOOTSTRAP_SERVER</span><span class="w">
</span><span class="w">              </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">my-kafka-cluster-kafka-bootstrap:9092</span><span class="w">
</span><span class="w">            </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">KAFKA_TOPIC</span><span class="w">
</span><span class="w">              </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">storm-events</span><span class="w">
</span><span class="w">            </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">SOURCE_FILE</span><span class="w">
</span><span class="w">              </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l">StormEvents.csv</span><span class="w">
</span></code></pre></div><p>To deploy the producer application:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl apply -f https://github.com/abhirockzz/kusto-kafka-connect-strimzi/raw/master/deploy/producer.yaml
</code></pre></div><p>The application picks up records from the <a href="https://github.com/abhirockzz/kusto-kafka-connect-strimzi/blob/master/storm-events-producer/StormEvents.csv">StormEvents.csv file</a> and sends them to a Kafka topic. Each event is a CSV record that represent data for a Storm occurence (start and end time, state, type etc.), for example: <code>2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23357,WISCONSIN,Winter Storm,COOP Observer</code>.</p>
<blockquote>
<p>The producer application <a href="https://github.com/abhirockzz/kusto-kafka-connect-strimzi/blob/master/storm-events-producer/main.go#L65">waits for 3 seconds</a> between subsequent produce operations to Kafka. This is intentional so that you can monitor the Kafka Connect logs and make sense of what&rsquo;s going on. The <code>StormEvents.csv</code> file contains more than 50,000 records, so it might take a while for all of them to be batched and ingested to Azure Data Explorer</p>
</blockquote>
<p>You can track the application logs using: <code>kubectl logs -f -l app=adx-event-producer</code>. If all is well, you should see something similar to this:</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">...
sent message to partition <span class="m">0</span> offset <span class="m">0</span>
event  2007-01-01 00:00:00.0000000,2007-01-01 00:00:00.0000000,13208,NORTH CAROLINA,Thunderstorm Wind,Public

sent message to partition <span class="m">0</span> offset <span class="m">1</span>
event  2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23358,WISCONSIN,Winter Storm,COOP Observer

sent message to partition <span class="m">0</span> offset <span class="m">2</span>
event  2007-01-01 00:00:00.0000000,2007-01-01 05:00:00.0000000,23357,WISCONSIN,Winter Storm,COOP Observer
</code></pre></div><p>The <code>storm-events</code> topic will now start getting events and these will be picked up by the sink connector. If you were to track the connector logs:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl logs -f -l strimzi.io/cluster<span class="o">=</span>my-connect-cluster
</code></pre></div><p>&hellip; you should see logs similar to this:</p>
<div class="highlight"><pre class="chroma"><code class="language-bash" data-lang="bash">....
INFO Kusto ingestion: file <span class="o">(</span>/tmp/kusto-sink-connector-17d03941-f8ca-498e-bc52-68ced036dc69/kafka_storm-events_0_0.csv.gz<span class="o">)</span> of size <span class="o">(</span>1722<span class="o">)</span> at current offset <span class="o">(</span>16<span class="o">)</span> <span class="o">(</span>com.microsoft.azure.kusto.kafka.connect.sink.TopicPartitionWriter<span class="o">)</span> <span class="o">[</span>Timer-6<span class="o">]</span>

INFO WorkerSinkTask<span class="o">{</span><span class="nv">id</span><span class="o">=</span>adx-sink-connector-0<span class="o">}</span> Committing offsets asynchronously using sequence number 17: <span class="o">{</span>storm-events-0<span class="o">=</span>OffsetAndMetadata<span class="o">{</span><span class="nv">offset</span><span class="o">=</span>17, <span class="nv">leaderEpoch</span><span class="o">=</span>null, <span class="nv">metadata</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="o">}}</span> <span class="o">(</span>org.apache.kafka.connect.runtime.WorkerSinkTask<span class="o">)</span> <span class="o">[</span>task-thread-adx-sink-connector-0<span class="o">]</span>

INFO Kusto ingestion: file <span class="o">(</span>/tmp/kusto-sink-connector-17d03941-f8ca-498e-bc52-68ced036dc69/kafka_storm-events_0_17.csv.gz<span class="o">)</span> of size <span class="o">(</span>1666<span class="o">)</span> at current offset <span class="o">(</span>33<span class="o">)</span> <span class="o">(</span>com.microsoft.azure.kusto.kafka.connect.sink.TopicPartitionWriter<span class="o">)</span> <span class="o">[</span>Timer-7<span class="o">]</span>
....
</code></pre></div><h3 id="query-azure-data-explorer">Query Azure Data Explorer</h3>
<p>Wait for sometime before data ends up in the <code>Storms</code> table. To confirm, check the row count and confirm that there are no failures in the ingestion process:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Storms <span class="p">|</span> count

. show ingestion failures
</code></pre></div><p>Once there is some data, try out a few queries. To see all the records:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Storms
</code></pre></div><p>Use <code>where</code> and <code>project</code> to filter specific data</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Storms
<span class="p">|</span> where <span class="nv">EventType</span> <span class="o">==</span> <span class="s1">&#39;Drought&#39;</span> and <span class="nv">State</span> <span class="o">==</span> <span class="s1">&#39;TEXAS&#39;</span>
<span class="p">|</span> project StartTime, EndTime, Source, EventId
</code></pre></div><p>Use the <a href="https://docs.microsoft.com/azure/data-explorer/write-queries?WT.mc_id=devto-blog-abhishgu#summarize"><code>summarize</code> operator</a></p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">Storms
<span class="p">|</span> summarize <span class="nv">event_count</span><span class="o">=</span>count<span class="o">()</span> by State
<span class="p">|</span> where event_count &gt; <span class="m">10</span>
<span class="p">|</span> project State, event_count
<span class="p">|</span> render columnchart
</code></pre></div><p><img src="https://strimzi.io/assets/images/posts/2020-09-22-columnchart.png" alt=""></p>
<p>These are just few examples. Please take a look at the <a href="https://docs.microsoft.com/azure/data-explorer/kusto/query/?WT.mc_id=devto-blog-abhishgu">Kusto Query Language documentation</a> or explore tutorials about how to ingest JSON formatted sample data into Azure Data Explorer, using <a href="https://docs.microsoft.com/azure/data-explorer/write-queries#scalar-operators">scalar operators</a>, <a href="https://docs.microsoft.com/azure/data-explorer/kusto/query/tutorial?pivots=azuredataexplorer&amp;WT.mc_id=devto-blog-abhishgu#timecharts">timecharts</a> etc.</p>
<h2 id="clean-up-resources">Clean up resources</h2>
<p>To delete the connector and/or Kafka cluster:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">kubectl delete kafkaconnect/my-connect-cluster
kubectl delete kafka/my-kafka-cluster
</code></pre></div><p>To delete the AKS and Azure Data Explorer clusters, simply delete the resource group:</p>
<div class="highlight"><pre class="chroma"><code class="language-shell" data-lang="shell">az group delete --name &lt;AZURE_RESOURCE_GROUP&gt; --yes --no-wait
</code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>That&rsquo;s all for this blog post and I hope you found it useful! Please note that, this is <em>not</em> the only way to ingest data into Azure Data Explorer. You&rsquo;re welcome to refer to the documentation and explore other techniques such as <a href="https://docs.microsoft.com/azure/data-explorer/ingest-data-one-click?WT.mc_id=devto-blog-abhishgu">One-click Ingestion</a>, using <a href="https://docs.microsoft.com/azure/data-explorer/ingest-data-event-grid-overview?WT.mc_id=devto-blog-abhishgu">Event Grid</a>, <a href="https://docs.microsoft.com/azure/data-explorer/ingest-data-iot-hub-overview?WT.mc_id=devto-blog-abhishgu">IoT Hub</a> etc.</p>
<p>Please consider exploring the following topics as additional learning resources:</p>
<h3 id="resources">Resources</h3>
<ul>
<li><a href="https://strimzi.io/docs/latest/#proc-configuring-kafka-connect-deployment-configuration-kafka-connect">Configuring Kafka Connect cluster</a> using Strimzi</li>
<li>Strimzi <a href="https://strimzi.io/docs/latest/#type-KafkaConnect-reference">KafkaConnect schema reference</a></li>
<li>Strimzi <a href="https://strimzi.io/docs/latest/#type-KafkaConnector-reference">KafkaConnector schema reference</a></li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/just-enough-azure-data-explorer-for-architects/ba-p/1636234">Just Enough Azure Data Explorer for Cloud Architects</a></li>
<li><a href="https://techcommunity.microsoft.com/t5/azure-data-explorer/azure-data-explorer-kafka-connector-new-features-with-version-1/ba-p/1637143">What&rsquo;s new in Azure Data Explorer connector 1.x</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/data-explorer/kql-quick-reference?WT.mc_id=devto-blog-abhishgu">Kusto Query Language</a></li>
</ul>


        

        

        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="https://abhirockzz.github.io/posts/cosmosdb-fault-tolerant-apps/" data-toggle="tooltip" data-placement="top" title="Build fault tolerant applications with Cassandra API for Azure Cosmos DB">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="https://abhirockzz.github.io/posts/postgres-azure-data-explorer-cdc/" data-toggle="tooltip" data-placement="top" title="Change Data Capture from PostgreSQL to Azure Data Explorer using Kafka Connect">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      

    </div>
  </div>
</div>

      
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
                <a href="https://github.com/abhirockzz" title="GitHub">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://twitter.com/abhi_tweeter" title="Twitter">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
                <a href="https://linkedin.com/in/abhirockzz" title="LinkedIn">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          <li>
            <a href="" title="RSS">
              <span class="fa-stack fa-lg">
                <i class="fas fa-circle fa-stack-2x"></i>
                <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
              </span>
            </a>
          </li>
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="https://abhirockzz.github.io">Abhishek Gupta</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2021
          

          
            &nbsp;&bull;&nbsp;
            <a href="https://abhirockzz.github.io/">Foo Bar Baz</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.80.0</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="https://abhirockzz.github.io/js/main.js"></script><script> renderMathInElement(document.body); </script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="https://abhirockzz.github.io/js/load-photoswipe.js"></script>









    
  </body>
</html>

